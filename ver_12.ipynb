{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "if 'ID' in train_df.columns:\n",
    "    train_df = train_df.drop(columns=['ID'])\n",
    "\n",
    "X_full = train_df.drop(columns=['Y'])\n",
    "y_full = train_df['Y'].values\n",
    "\n",
    "X_filled = X_full.fillna(X_full.median(numeric_only=True))\n",
    "X_values = X_filled.values\n",
    "mu = X_values.mean(axis=0)\n",
    "sigma = X_values.std(axis=0) + 1e-8\n",
    "std = lambda a: (a - mu) / sigma\n",
    "X_base = std(X_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.n_estimators = 400\n",
    "        self.max_depth = 20\n",
    "        self.min_samples_split = 2\n",
    "        self.min_samples_leaf = 1\n",
    "        self.max_features = 'sqrt'\n",
    "        self.bootstrap = True\n",
    "        self.trees = []\n",
    "        self.oob_indices = []\n",
    "        self.feature_importance_ = None\n",
    "        self.patience = 10\n",
    "        self.best_oob = -1\n",
    "        self.no_improve = 0\n",
    "\n",
    "    def _gini_impurity(self, y):\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        p = np.sum(y == 1) / len(y)\n",
    "        return 2 * p * (1 - p)\n",
    "\n",
    "    def _information_gain(self, y, left_y, right_y):\n",
    "        n = len(y)\n",
    "        n_left, n_right = len(left_y), len(right_y)\n",
    "        if n == 0:\n",
    "            return 0\n",
    "        parent_gini = self._gini_impurity(y)\n",
    "        left_gini = self._gini_impurity(left_y)\n",
    "        right_gini = self._gini_impurity(right_y)\n",
    "        weighted_gini = (n_left / n) * left_gini + (n_right / n) * right_gini\n",
    "        return parent_gini - weighted_gini\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0, importance_tracker=None):\n",
    "        n_samples, n_features = X.shape\n",
    "        if (depth >= self.max_depth or\n",
    "            n_samples < self.min_samples_split or\n",
    "            len(np.unique(y)) == 1):\n",
    "            return {'leaf': True, 'prediction': np.round(np.mean(y))}\n",
    "\n",
    "        max_features = int(np.sqrt(n_features)) if self.max_features == 'sqrt' else n_features\n",
    "        feature_indices = np.random.choice(n_features, max_features, replace=False)\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature_idx in feature_indices:\n",
    "            thresholds = np.percentile(X[:, feature_idx], [10,25,50,75,90])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature_idx] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                if np.sum(left_mask) < self.min_samples_leaf or np.sum(right_mask) < self.min_samples_leaf:\n",
    "                    continue\n",
    "                gain = self._information_gain(y, y[left_mask], y[right_mask])\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        if best_feature is None:\n",
    "            return {'leaf': True, 'prediction': np.round(np.mean(y))}\n",
    "\n",
    "        if importance_tracker is not None:\n",
    "            importance_tracker[best_feature] += best_gain\n",
    "\n",
    "        left_mask = X[:, best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "        left_tree = self._build_tree(X[left_mask], y[left_mask], depth + 1, importance_tracker)\n",
    "        right_tree = self._build_tree(X[right_mask], y[right_mask], depth + 1, importance_tracker)\n",
    "        return {\n",
    "            'leaf': False,\n",
    "            'feature': best_feature,\n",
    "            'threshold': best_threshold,\n",
    "            'left': left_tree,\n",
    "            'right': right_tree\n",
    "        }\n",
    "\n",
    "    def _predict_tree(self, tree, X):\n",
    "        if tree['leaf']:\n",
    "            return np.full(len(X), tree['prediction'])\n",
    "        predictions = np.zeros(len(X))\n",
    "        left_mask = X[:, tree['feature']] <= tree['threshold']\n",
    "        right_mask = ~left_mask\n",
    "        if np.any(left_mask):\n",
    "            predictions[left_mask] = self._predict_tree(tree['left'], X[left_mask])\n",
    "        if np.any(right_mask):\n",
    "            predictions[right_mask] = self._predict_tree(tree['right'], X[right_mask])\n",
    "        return predictions\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print(f\"Training with OOB-based Early Stopping\")\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self.oob_indices = []\n",
    "        self.trees = []\n",
    "        self.best_oob = -1\n",
    "        self.no_improve = 0\n",
    "\n",
    "        total_importance = np.zeros(n_features)\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            oob_idx = np.setdiff1d(np.arange(n_samples), indices)\n",
    "            self.oob_indices.append(oob_idx)\n",
    "\n",
    "            X_bootstrap, y_bootstrap = X[indices], y[indices]\n",
    "            importance_tracker = np.zeros(n_features)\n",
    "            tree = self._build_tree(X_bootstrap, y_bootstrap, importance_tracker=importance_tracker)\n",
    "            self.trees.append(tree)\n",
    "            total_importance += importance_tracker\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                oob_preds = self._get_oob_predictions(X, i + 1)\n",
    "                oob_acc = np.mean(oob_preds == y)\n",
    "                print(f\"[{i + 1}] OOB Accuracy: {oob_acc*100:.2f}%\")\n",
    "\n",
    "                if oob_acc > self.best_oob + 1e-6:\n",
    "                    self.best_oob = oob_acc\n",
    "                    self.no_improve = 0\n",
    "                else:\n",
    "                    self.no_improve += 1\n",
    "                    if self.no_improve >= self.patience:\n",
    "                        print(f\"Early stopping at tree {i + 1} (OOB Accuracy stagnated)\")\n",
    "                        break\n",
    "\n",
    "        total = np.sum(total_importance)\n",
    "        self.feature_importance_ = total_importance / total if total > 0 else np.zeros(n_features)\n",
    "        print(\"Model training completed.\\n\")\n",
    "\n",
    "\n",
    "    def _get_oob_predictions(self, X, n_trees):\n",
    "        n_samples = X.shape[0]\n",
    "        votes = np.zeros(n_samples)\n",
    "        counts = np.zeros(n_samples)\n",
    "        for i in range(n_trees):\n",
    "            oob_idx = self.oob_indices[i]\n",
    "            if len(oob_idx) == 0:\n",
    "                continue\n",
    "            preds = self._predict_tree(self.trees[i], X[oob_idx])\n",
    "            votes[oob_idx] += preds\n",
    "            counts[oob_idx] += 1\n",
    "        final = np.zeros(n_samples)\n",
    "        mask = counts > 0\n",
    "        final[mask] = (votes[mask] / counts[mask] > 0.5).astype(int)\n",
    "        return final\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        for tree in self.trees:\n",
    "            predictions += self._predict_tree(tree, X)\n",
    "        return predictions / len(self.trees)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) > 0.5).astype(int)\n",
    "\n",
    "\n",
    "def k_fold_indices(n_samples, k, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    indices = rng.permutation(n_samples)\n",
    "    fold_sizes = [n_samples // k] * k\n",
    "    for i in range(n_samples % k):\n",
    "        fold_sizes[i] += 1\n",
    "    folds, current = [], 0\n",
    "    for size in fold_sizes:\n",
    "        folds.append(indices[current: current + size])\n",
    "        current += size\n",
    "    return folds\n",
    "\n",
    "def cross_val_score(X, y, params, k=3):\n",
    "    folds = k_fold_indices(len(X), k)\n",
    "    scores = []\n",
    "    print(f\"Cross-validation (k={k}) started...\")\n",
    "    for i in range(k):\n",
    "        print(f\"Fold {i + 1}/{k}\")\n",
    "        val_idx = folds[i]\n",
    "        train_idx = np.hstack([folds[j] for j in range(k) if j != i])\n",
    "        model = Model()\n",
    "        for key, value in params.items():\n",
    "            setattr(model, key, value)\n",
    "        model.fit(X[train_idx], y[train_idx])\n",
    "        preds = model.predict(X[val_idx])\n",
    "        acc = np.mean(preds == y[val_idx])\n",
    "        scores.append(acc)\n",
    "        print(f\"Fold {i + 1} Accuracy: {acc*100:.2f}%\")\n",
    "    mean_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    print(f\"\\nCV Mean Accuracy: {mean_score*100:.2f}% Â± {std_score*100:.2f}%\\n\")\n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with OOB-based Early Stopping\n",
      "[10] OOB Accuracy: 64.08%\n",
      "[20] OOB Accuracy: 68.42%\n",
      "[30] OOB Accuracy: 70.55%\n",
      "[40] OOB Accuracy: 71.66%\n",
      "[50] OOB Accuracy: 72.17%\n",
      "[60] OOB Accuracy: 72.79%\n",
      "[70] OOB Accuracy: 73.06%\n",
      "[80] OOB Accuracy: 73.34%\n",
      "[90] OOB Accuracy: 73.79%\n",
      "[100] OOB Accuracy: 73.96%\n",
      "[110] OOB Accuracy: 74.24%\n",
      "[120] OOB Accuracy: 74.46%\n",
      "[130] OOB Accuracy: 74.78%\n",
      "[140] OOB Accuracy: 75.02%\n",
      "[150] OOB Accuracy: 75.06%\n",
      "[160] OOB Accuracy: 75.17%\n",
      "[170] OOB Accuracy: 75.17%\n",
      "[180] OOB Accuracy: 75.31%\n",
      "[190] OOB Accuracy: 75.52%\n",
      "[200] OOB Accuracy: 75.48%\n",
      "[210] OOB Accuracy: 75.42%\n",
      "[220] OOB Accuracy: 75.52%\n",
      "[230] OOB Accuracy: 75.49%\n",
      "[240] OOB Accuracy: 75.90%\n",
      "[250] OOB Accuracy: 76.01%\n",
      "[260] OOB Accuracy: 76.04%\n",
      "[270] OOB Accuracy: 76.29%\n",
      "[280] OOB Accuracy: 76.11%\n",
      "[290] OOB Accuracy: 76.14%\n",
      "[300] OOB Accuracy: 76.24%\n",
      "[310] OOB Accuracy: 76.17%\n",
      "[320] OOB Accuracy: 76.06%\n",
      "[330] OOB Accuracy: 76.05%\n",
      "[340] OOB Accuracy: 76.12%\n",
      "[350] OOB Accuracy: 76.20%\n",
      "[360] OOB Accuracy: 76.15%\n",
      "[370] OOB Accuracy: 76.19%\n",
      "Early stopping at tree 370 (OOB Accuracy stagnated)\n",
      "Model training completed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "correlated_pairs = [(4, 9), (3, 9), (10, 16), (11, 16), (4, 10), (6, 8)]\n",
    "\n",
    "def add_interactions_combined(X, important_indices, correlated_pairs):\n",
    "    X_new = X.copy()\n",
    "\n",
    "    for i in range(len(important_indices)):\n",
    "        for j in range(i + 1, len(important_indices)):\n",
    "            fi, fj = important_indices[i], important_indices[j]\n",
    "            X_new = np.column_stack([X_new, X[:, fi] * X[:, fj]])\n",
    "\n",
    "    for i in range(len(important_indices)):\n",
    "        for j in range(len(important_indices)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            fi, fj = important_indices[i], important_indices[j]\n",
    "            X_new = np.column_stack([X_new, X[:, fi] / (X[:, fj] + 1e-8)])\n",
    "\n",
    "    for i, j in correlated_pairs:\n",
    "        X_new = np.column_stack([X_new, X[:, i] * X[:, j]])\n",
    "\n",
    "    return X_new\n",
    "\n",
    "init_model = Model()\n",
    "init_model.max_depth = 15\n",
    "init_model.fit(X_base, y_full)\n",
    "init_importance = init_model.feature_importance_\n",
    "top_feats = np.argsort(init_importance)[::-1][:5]\n",
    "\n",
    "X_eng = add_interactions_combined(X_base, top_feats, correlated_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with OOB-based Early Stopping\n",
      "[10] OOB Accuracy: 68.84%\n",
      "[20] OOB Accuracy: 72.00%\n",
      "[30] OOB Accuracy: 72.91%\n",
      "[40] OOB Accuracy: 74.00%\n",
      "[50] OOB Accuracy: 74.41%\n",
      "[60] OOB Accuracy: 75.02%\n",
      "[70] OOB Accuracy: 75.56%\n",
      "[80] OOB Accuracy: 75.61%\n",
      "[90] OOB Accuracy: 75.48%\n",
      "[100] OOB Accuracy: 75.66%\n",
      "[110] OOB Accuracy: 75.88%\n",
      "[120] OOB Accuracy: 76.10%\n",
      "[130] OOB Accuracy: 76.16%\n",
      "[140] OOB Accuracy: 76.17%\n",
      "[150] OOB Accuracy: 76.15%\n",
      "[160] OOB Accuracy: 76.38%\n",
      "[170] OOB Accuracy: 76.49%\n",
      "[180] OOB Accuracy: 76.59%\n",
      "[190] OOB Accuracy: 76.74%\n",
      "[200] OOB Accuracy: 76.69%\n",
      "[210] OOB Accuracy: 76.62%\n",
      "[220] OOB Accuracy: 76.65%\n",
      "[230] OOB Accuracy: 76.72%\n",
      "[240] OOB Accuracy: 76.72%\n",
      "[250] OOB Accuracy: 76.65%\n",
      "[260] OOB Accuracy: 76.84%\n",
      "[270] OOB Accuracy: 77.01%\n",
      "[280] OOB Accuracy: 76.95%\n",
      "[290] OOB Accuracy: 76.94%\n",
      "[300] OOB Accuracy: 76.91%\n",
      "[310] OOB Accuracy: 76.98%\n",
      "[320] OOB Accuracy: 77.04%\n",
      "[330] OOB Accuracy: 76.98%\n",
      "[340] OOB Accuracy: 76.85%\n",
      "[350] OOB Accuracy: 76.90%\n",
      "[360] OOB Accuracy: 76.78%\n",
      "[370] OOB Accuracy: 76.86%\n",
      "[380] OOB Accuracy: 76.88%\n",
      "[390] OOB Accuracy: 76.94%\n",
      "[400] OOB Accuracy: 77.04%\n",
      "Model training completed.\n",
      "\n",
      "Cross-validation (k=5) started...\n",
      "   Fold 1/5\n",
      "Training with OOB-based Early Stopping\n",
      "[10] OOB Accuracy: 67.91%\n",
      "[20] OOB Accuracy: 70.50%\n",
      "[30] OOB Accuracy: 72.58%\n",
      "[40] OOB Accuracy: 73.17%\n",
      "[50] OOB Accuracy: 73.73%\n",
      "[60] OOB Accuracy: 74.59%\n",
      "[70] OOB Accuracy: 74.86%\n",
      "[80] OOB Accuracy: 75.09%\n",
      "[90] OOB Accuracy: 75.09%\n",
      "[100] OOB Accuracy: 75.11%\n",
      "[110] OOB Accuracy: 75.45%\n",
      "[120] OOB Accuracy: 75.59%\n",
      "[130] OOB Accuracy: 75.47%\n",
      "[140] OOB Accuracy: 75.58%\n",
      "[150] OOB Accuracy: 75.52%\n",
      "[160] OOB Accuracy: 75.61%\n",
      "[170] OOB Accuracy: 75.61%\n",
      "[180] OOB Accuracy: 75.75%\n",
      "[190] OOB Accuracy: 75.81%\n",
      "[200] OOB Accuracy: 76.05%\n",
      "[210] OOB Accuracy: 75.95%\n",
      "[220] OOB Accuracy: 76.00%\n",
      "[230] OOB Accuracy: 75.94%\n",
      "[240] OOB Accuracy: 76.02%\n",
      "[250] OOB Accuracy: 76.05%\n",
      "[260] OOB Accuracy: 76.00%\n",
      "[270] OOB Accuracy: 75.88%\n",
      "[280] OOB Accuracy: 76.08%\n",
      "[290] OOB Accuracy: 76.16%\n",
      "[300] OOB Accuracy: 76.03%\n",
      "[310] OOB Accuracy: 76.05%\n",
      "[320] OOB Accuracy: 76.00%\n",
      "[330] OOB Accuracy: 75.91%\n",
      "[340] OOB Accuracy: 75.95%\n",
      "[350] OOB Accuracy: 76.00%\n",
      "[360] OOB Accuracy: 76.14%\n",
      "[370] OOB Accuracy: 76.11%\n",
      "[380] OOB Accuracy: 76.22%\n",
      "[390] OOB Accuracy: 76.17%\n",
      "[400] OOB Accuracy: 76.14%\n",
      "Model training completed.\n",
      "\n",
      "     Fold 1 Accuracy: 77.69%\n",
      "   Fold 2/5\n",
      "Training with OOB-based Early Stopping\n",
      "[10] OOB Accuracy: 67.84%\n",
      "[20] OOB Accuracy: 71.28%\n",
      "[30] OOB Accuracy: 72.75%\n",
      "[40] OOB Accuracy: 73.70%\n",
      "[50] OOB Accuracy: 74.56%\n",
      "[60] OOB Accuracy: 74.83%\n",
      "[70] OOB Accuracy: 75.00%\n",
      "[80] OOB Accuracy: 75.05%\n",
      "[90] OOB Accuracy: 75.48%\n",
      "[100] OOB Accuracy: 75.98%\n",
      "[110] OOB Accuracy: 75.94%\n",
      "[120] OOB Accuracy: 75.97%\n",
      "[130] OOB Accuracy: 76.14%\n",
      "[140] OOB Accuracy: 76.23%\n",
      "[150] OOB Accuracy: 76.30%\n",
      "[160] OOB Accuracy: 76.45%\n",
      "[170] OOB Accuracy: 76.39%\n",
      "[180] OOB Accuracy: 76.30%\n",
      "[190] OOB Accuracy: 76.28%\n",
      "[200] OOB Accuracy: 76.28%\n",
      "[210] OOB Accuracy: 76.36%\n",
      "[220] OOB Accuracy: 76.22%\n",
      "[230] OOB Accuracy: 76.25%\n",
      "[240] OOB Accuracy: 76.42%\n",
      "[250] OOB Accuracy: 76.33%\n",
      "[260] OOB Accuracy: 76.42%\n",
      "Early stopping at tree 260 (OOB Accuracy stagnated)\n",
      "Model training completed.\n",
      "\n",
      "     Fold 2 Accuracy: 77.00%\n",
      "   Fold 3/5\n",
      "Training with OOB-based Early Stopping\n",
      "[10] OOB Accuracy: 67.41%\n",
      "[20] OOB Accuracy: 70.48%\n",
      "[30] OOB Accuracy: 72.78%\n",
      "[40] OOB Accuracy: 73.45%\n",
      "[50] OOB Accuracy: 73.80%\n",
      "[60] OOB Accuracy: 74.58%\n",
      "[70] OOB Accuracy: 74.98%\n",
      "[80] OOB Accuracy: 75.12%\n",
      "[90] OOB Accuracy: 75.42%\n",
      "[100] OOB Accuracy: 75.41%\n",
      "[110] OOB Accuracy: 75.75%\n",
      "[120] OOB Accuracy: 76.20%\n",
      "[130] OOB Accuracy: 76.22%\n",
      "[140] OOB Accuracy: 76.23%\n",
      "[150] OOB Accuracy: 76.14%\n",
      "[160] OOB Accuracy: 76.11%\n",
      "[170] OOB Accuracy: 76.33%\n",
      "[180] OOB Accuracy: 76.19%\n",
      "[190] OOB Accuracy: 76.38%\n",
      "[200] OOB Accuracy: 76.53%\n",
      "[210] OOB Accuracy: 76.39%\n",
      "[220] OOB Accuracy: 76.48%\n",
      "[230] OOB Accuracy: 76.69%\n",
      "[240] OOB Accuracy: 76.62%\n",
      "[250] OOB Accuracy: 76.52%\n",
      "[260] OOB Accuracy: 76.53%\n",
      "[270] OOB Accuracy: 76.56%\n",
      "[280] OOB Accuracy: 76.50%\n",
      "[290] OOB Accuracy: 76.70%\n",
      "[300] OOB Accuracy: 76.59%\n",
      "[310] OOB Accuracy: 76.45%\n",
      "[320] OOB Accuracy: 76.30%\n",
      "[330] OOB Accuracy: 76.33%\n",
      "[340] OOB Accuracy: 76.33%\n",
      "[350] OOB Accuracy: 76.48%\n",
      "[360] OOB Accuracy: 76.48%\n",
      "[370] OOB Accuracy: 76.45%\n",
      "[380] OOB Accuracy: 76.41%\n",
      "[390] OOB Accuracy: 76.39%\n",
      "Early stopping at tree 390 (OOB Accuracy stagnated)\n",
      "Model training completed.\n",
      "\n",
      "     Fold 3 Accuracy: 75.88%\n",
      "   Fold 4/5\n",
      "Training with OOB-based Early Stopping\n",
      "[10] OOB Accuracy: 67.69%\n",
      "[20] OOB Accuracy: 71.38%\n",
      "[30] OOB Accuracy: 72.62%\n",
      "[40] OOB Accuracy: 73.64%\n",
      "[50] OOB Accuracy: 74.55%\n",
      "[60] OOB Accuracy: 74.81%\n",
      "[70] OOB Accuracy: 75.50%\n",
      "[80] OOB Accuracy: 76.05%\n",
      "[90] OOB Accuracy: 76.28%\n",
      "[100] OOB Accuracy: 76.33%\n",
      "[110] OOB Accuracy: 76.39%\n",
      "[120] OOB Accuracy: 76.22%\n",
      "[130] OOB Accuracy: 76.23%\n",
      "[140] OOB Accuracy: 76.39%\n",
      "[150] OOB Accuracy: 76.44%\n",
      "[160] OOB Accuracy: 76.83%\n",
      "[170] OOB Accuracy: 76.73%\n",
      "[180] OOB Accuracy: 76.77%\n",
      "[190] OOB Accuracy: 76.72%\n",
      "[200] OOB Accuracy: 76.75%\n",
      "[210] OOB Accuracy: 76.81%\n",
      "[220] OOB Accuracy: 76.72%\n",
      "[230] OOB Accuracy: 76.70%\n",
      "[240] OOB Accuracy: 76.50%\n",
      "[250] OOB Accuracy: 76.34%\n",
      "[260] OOB Accuracy: 76.41%\n",
      "Early stopping at tree 260 (OOB Accuracy stagnated)\n",
      "Model training completed.\n",
      "\n",
      "     Fold 4 Accuracy: 77.00%\n",
      "   Fold 5/5\n",
      "Training with OOB-based Early Stopping\n",
      "[10] OOB Accuracy: 67.11%\n",
      "[20] OOB Accuracy: 71.47%\n",
      "[30] OOB Accuracy: 73.23%\n",
      "[40] OOB Accuracy: 73.89%\n",
      "[50] OOB Accuracy: 74.61%\n",
      "[60] OOB Accuracy: 74.73%\n",
      "[70] OOB Accuracy: 75.33%\n",
      "[80] OOB Accuracy: 75.81%\n",
      "[90] OOB Accuracy: 75.78%\n",
      "[100] OOB Accuracy: 75.81%\n",
      "[110] OOB Accuracy: 76.17%\n",
      "[120] OOB Accuracy: 76.28%\n",
      "[130] OOB Accuracy: 76.17%\n",
      "[140] OOB Accuracy: 76.33%\n",
      "[150] OOB Accuracy: 76.38%\n",
      "[160] OOB Accuracy: 76.38%\n",
      "[170] OOB Accuracy: 76.36%\n",
      "[180] OOB Accuracy: 76.19%\n",
      "[190] OOB Accuracy: 76.42%\n",
      "[200] OOB Accuracy: 76.11%\n",
      "[210] OOB Accuracy: 76.36%\n",
      "[220] OOB Accuracy: 76.36%\n",
      "[230] OOB Accuracy: 76.44%\n",
      "[240] OOB Accuracy: 76.45%\n",
      "[250] OOB Accuracy: 76.44%\n",
      "[260] OOB Accuracy: 76.42%\n",
      "[270] OOB Accuracy: 76.64%\n",
      "[280] OOB Accuracy: 76.41%\n",
      "[290] OOB Accuracy: 76.45%\n",
      "[300] OOB Accuracy: 76.58%\n",
      "[310] OOB Accuracy: 76.53%\n",
      "[320] OOB Accuracy: 76.53%\n",
      "[330] OOB Accuracy: 76.61%\n",
      "[340] OOB Accuracy: 76.78%\n",
      "[350] OOB Accuracy: 76.67%\n",
      "[360] OOB Accuracy: 76.67%\n",
      "[370] OOB Accuracy: 76.53%\n",
      "[380] OOB Accuracy: 76.61%\n",
      "[390] OOB Accuracy: 76.73%\n",
      "[400] OOB Accuracy: 76.72%\n",
      "Model training completed.\n",
      "\n",
      "     Fold 5 Accuracy: 77.00%\n",
      "\n",
      " CV Mean Accuracy: 76.91% Â± 0.58%\n",
      "\n",
      "k=40 depth=20 split=2 leaf=1 -> CV Acc 76.91%\n",
      "Cross-validation (k=5) started...\n",
      "   Fold 1/5\n",
      "Training with OOB-based Early Stopping\n",
      "[10] OOB Accuracy: 66.64%\n",
      "[20] OOB Accuracy: 70.06%\n",
      "[30] OOB Accuracy: 71.66%\n",
      "[40] OOB Accuracy: 72.05%\n",
      "[50] OOB Accuracy: 72.84%\n",
      "[60] OOB Accuracy: 73.30%\n",
      "[70] OOB Accuracy: 74.17%\n",
      "[80] OOB Accuracy: 73.97%\n",
      "[90] OOB Accuracy: 74.19%\n",
      "[100] OOB Accuracy: 74.08%\n",
      "[110] OOB Accuracy: 74.45%\n",
      "[120] OOB Accuracy: 74.52%\n",
      "[130] OOB Accuracy: 74.44%\n",
      "[140] OOB Accuracy: 74.70%\n",
      "[150] OOB Accuracy: 74.98%\n",
      "[160] OOB Accuracy: 74.70%\n",
      "[170] OOB Accuracy: 74.80%\n",
      "[180] OOB Accuracy: 75.09%\n",
      "[190] OOB Accuracy: 75.09%\n",
      "[200] OOB Accuracy: 75.05%\n",
      "[210] OOB Accuracy: 75.30%\n",
      "[220] OOB Accuracy: 75.56%\n",
      "[230] OOB Accuracy: 75.30%\n",
      "[240] OOB Accuracy: 75.53%\n",
      "[250] OOB Accuracy: 75.67%\n",
      "[260] OOB Accuracy: 75.75%\n",
      "[270] OOB Accuracy: 75.56%\n",
      "[280] OOB Accuracy: 75.73%\n",
      "[290] OOB Accuracy: 75.61%\n",
      "[300] OOB Accuracy: 75.34%\n",
      "[310] OOB Accuracy: 75.64%\n",
      "[320] OOB Accuracy: 75.77%\n",
      "[330] OOB Accuracy: 75.81%\n",
      "[340] OOB Accuracy: 75.55%\n",
      "[350] OOB Accuracy: 75.69%\n",
      "[360] OOB Accuracy: 75.56%\n",
      "[370] OOB Accuracy: 75.59%\n",
      "[380] OOB Accuracy: 75.80%\n",
      "[390] OOB Accuracy: 75.83%\n",
      "[400] OOB Accuracy: 75.81%\n",
      "Model training completed.\n",
      "\n",
      "     Fold 1 Accuracy: 78.19%\n",
      "   Fold 2/5\n",
      "Training with OOB-based Early Stopping\n",
      "[10] OOB Accuracy: 65.66%\n",
      "[20] OOB Accuracy: 69.97%\n",
      "[30] OOB Accuracy: 71.31%\n",
      "[40] OOB Accuracy: 72.45%\n",
      "[50] OOB Accuracy: 72.91%\n",
      "[60] OOB Accuracy: 73.58%\n",
      "[70] OOB Accuracy: 74.05%\n",
      "[80] OOB Accuracy: 74.45%\n",
      "[90] OOB Accuracy: 74.92%\n",
      "[100] OOB Accuracy: 75.00%\n",
      "[110] OOB Accuracy: 75.16%\n",
      "[120] OOB Accuracy: 74.98%\n",
      "[130] OOB Accuracy: 74.95%\n",
      "[140] OOB Accuracy: 75.41%\n",
      "[150] OOB Accuracy: 75.45%\n",
      "[160] OOB Accuracy: 75.36%\n",
      "[170] OOB Accuracy: 75.31%\n",
      "[180] OOB Accuracy: 75.39%\n",
      "[190] OOB Accuracy: 75.36%\n",
      "[200] OOB Accuracy: 75.44%\n",
      "[210] OOB Accuracy: 75.39%\n",
      "[220] OOB Accuracy: 75.58%\n",
      "[230] OOB Accuracy: 75.55%\n",
      "[240] OOB Accuracy: 75.59%\n",
      "[250] OOB Accuracy: 75.67%\n",
      "[260] OOB Accuracy: 75.92%\n",
      "[270] OOB Accuracy: 75.80%\n",
      "[280] OOB Accuracy: 75.72%\n",
      "[290] OOB Accuracy: 75.77%\n",
      "[300] OOB Accuracy: 75.75%\n",
      "[310] OOB Accuracy: 75.58%\n",
      "[320] OOB Accuracy: 75.66%\n",
      "[330] OOB Accuracy: 75.84%\n",
      "[340] OOB Accuracy: 75.80%\n",
      "[350] OOB Accuracy: 76.02%\n",
      "[360] OOB Accuracy: 75.97%\n",
      "[370] OOB Accuracy: 75.95%\n",
      "[380] OOB Accuracy: 75.97%\n",
      "[390] OOB Accuracy: 75.83%\n",
      "[400] OOB Accuracy: 75.94%\n",
      "Model training completed.\n",
      "\n",
      "     Fold 2 Accuracy: 76.12%\n",
      "   Fold 3/5\n",
      "Training with OOB-based Early Stopping\n",
      "[10] OOB Accuracy: 66.98%\n",
      "[20] OOB Accuracy: 71.19%\n",
      "[30] OOB Accuracy: 73.06%\n",
      "[40] OOB Accuracy: 73.72%\n",
      "[50] OOB Accuracy: 73.56%\n",
      "[60] OOB Accuracy: 74.53%\n",
      "[70] OOB Accuracy: 74.67%\n",
      "[80] OOB Accuracy: 74.53%\n",
      "[90] OOB Accuracy: 74.73%\n",
      "[100] OOB Accuracy: 75.11%\n",
      "[110] OOB Accuracy: 75.27%\n",
      "[120] OOB Accuracy: 75.38%\n",
      "[130] OOB Accuracy: 75.48%\n",
      "[140] OOB Accuracy: 75.53%\n",
      "[150] OOB Accuracy: 75.67%\n",
      "[160] OOB Accuracy: 75.83%\n",
      "[170] OOB Accuracy: 75.77%\n",
      "[180] OOB Accuracy: 76.02%\n",
      "[190] OOB Accuracy: 76.02%\n",
      "[200] OOB Accuracy: 76.02%\n",
      "[210] OOB Accuracy: 76.12%\n",
      "[220] OOB Accuracy: 76.11%\n",
      "[230] OOB Accuracy: 76.16%\n",
      "[240] OOB Accuracy: 76.14%\n",
      "[250] OOB Accuracy: 76.28%\n",
      "[260] OOB Accuracy: 76.48%\n",
      "[270] OOB Accuracy: 76.33%\n",
      "[280] OOB Accuracy: 76.20%\n",
      "[290] OOB Accuracy: 76.25%\n",
      "[300] OOB Accuracy: 76.36%\n",
      "[310] OOB Accuracy: 76.34%\n",
      "[320] OOB Accuracy: 76.44%\n",
      "[330] OOB Accuracy: 76.39%\n",
      "[340] OOB Accuracy: 76.39%\n",
      "[350] OOB Accuracy: 76.28%\n",
      "[360] OOB Accuracy: 76.25%\n",
      "Early stopping at tree 360 (OOB Accuracy stagnated)\n",
      "Model training completed.\n",
      "\n",
      "     Fold 3 Accuracy: 75.31%\n",
      "   Fold 4/5\n",
      "Training with OOB-based Early Stopping\n",
      "[10] OOB Accuracy: 67.33%\n",
      "[20] OOB Accuracy: 70.19%\n",
      "[30] OOB Accuracy: 72.58%\n",
      "[40] OOB Accuracy: 72.81%\n",
      "[50] OOB Accuracy: 73.17%\n",
      "[60] OOB Accuracy: 73.80%\n",
      "[70] OOB Accuracy: 74.14%\n",
      "[80] OOB Accuracy: 74.72%\n",
      "[90] OOB Accuracy: 74.88%\n",
      "[100] OOB Accuracy: 74.62%\n",
      "[110] OOB Accuracy: 74.84%\n",
      "[120] OOB Accuracy: 74.92%\n",
      "[130] OOB Accuracy: 75.23%\n",
      "[140] OOB Accuracy: 75.34%\n",
      "[150] OOB Accuracy: 75.16%\n",
      "[160] OOB Accuracy: 75.56%\n",
      "[170] OOB Accuracy: 75.58%\n",
      "[180] OOB Accuracy: 75.61%\n",
      "[190] OOB Accuracy: 75.38%\n",
      "[200] OOB Accuracy: 75.59%\n",
      "[210] OOB Accuracy: 75.50%\n",
      "[220] OOB Accuracy: 75.38%\n",
      "[230] OOB Accuracy: 75.44%\n",
      "[240] OOB Accuracy: 75.62%\n",
      "[250] OOB Accuracy: 75.81%\n",
      "[260] OOB Accuracy: 75.83%\n",
      "[270] OOB Accuracy: 75.94%\n",
      "[280] OOB Accuracy: 75.88%\n",
      "[290] OOB Accuracy: 76.02%\n",
      "[300] OOB Accuracy: 76.05%\n",
      "[310] OOB Accuracy: 75.95%\n",
      "[320] OOB Accuracy: 75.91%\n",
      "[330] OOB Accuracy: 76.11%\n",
      "[340] OOB Accuracy: 75.98%\n",
      "[350] OOB Accuracy: 76.00%\n",
      "[360] OOB Accuracy: 76.03%\n",
      "[370] OOB Accuracy: 76.09%\n",
      "[380] OOB Accuracy: 76.05%\n",
      "[390] OOB Accuracy: 76.14%\n",
      "[400] OOB Accuracy: 76.09%\n",
      "Model training completed.\n",
      "\n",
      "     Fold 4 Accuracy: 74.38%\n",
      "   Fold 5/5\n",
      "Training with OOB-based Early Stopping\n",
      "[10] OOB Accuracy: 67.28%\n",
      "[20] OOB Accuracy: 71.11%\n",
      "[30] OOB Accuracy: 72.17%\n",
      "[40] OOB Accuracy: 72.78%\n",
      "[50] OOB Accuracy: 73.78%\n",
      "[60] OOB Accuracy: 74.11%\n",
      "[70] OOB Accuracy: 74.42%\n",
      "[80] OOB Accuracy: 74.67%\n",
      "[90] OOB Accuracy: 74.61%\n",
      "[100] OOB Accuracy: 75.22%\n",
      "[110] OOB Accuracy: 75.33%\n",
      "[120] OOB Accuracy: 75.56%\n",
      "[130] OOB Accuracy: 75.55%\n",
      "[140] OOB Accuracy: 75.61%\n",
      "[150] OOB Accuracy: 75.67%\n",
      "[160] OOB Accuracy: 75.81%\n",
      "[170] OOB Accuracy: 75.77%\n",
      "[180] OOB Accuracy: 75.62%\n",
      "[190] OOB Accuracy: 75.59%\n",
      "[200] OOB Accuracy: 75.17%\n",
      "[210] OOB Accuracy: 75.44%\n",
      "[220] OOB Accuracy: 75.42%\n",
      "[230] OOB Accuracy: 75.27%\n",
      "[240] OOB Accuracy: 75.36%\n",
      "[250] OOB Accuracy: 75.34%\n",
      "[260] OOB Accuracy: 75.38%\n",
      "Early stopping at tree 260 (OOB Accuracy stagnated)\n",
      "Model training completed.\n",
      "\n",
      "     Fold 5 Accuracy: 76.38%\n",
      "\n",
      " CV Mean Accuracy: 76.07% Â± 1.27%\n",
      "\n",
      "k=40 depth=20 split=2 leaf=2 -> CV Acc 76.07%\n",
      "Cross-validation (k=5) started...\n",
      "   Fold 1/5\n",
      "Training with OOB-based Early Stopping\n",
      "[10] OOB Accuracy: 66.22%\n",
      "[20] OOB Accuracy: 71.20%\n",
      "[30] OOB Accuracy: 72.66%\n",
      "[40] OOB Accuracy: 73.55%\n",
      "[50] OOB Accuracy: 73.80%\n",
      "[60] OOB Accuracy: 73.91%\n",
      "[70] OOB Accuracy: 74.47%\n",
      "[80] OOB Accuracy: 75.05%\n",
      "[90] OOB Accuracy: 75.12%\n",
      "[100] OOB Accuracy: 75.33%\n",
      "[110] OOB Accuracy: 75.41%\n",
      "[120] OOB Accuracy: 75.48%\n",
      "[130] OOB Accuracy: 75.77%\n",
      "[140] OOB Accuracy: 76.02%\n",
      "[150] OOB Accuracy: 75.88%\n",
      "[160] OOB Accuracy: 75.89%\n",
      "[170] OOB Accuracy: 75.78%\n",
      "[180] OOB Accuracy: 75.95%\n",
      "[190] OOB Accuracy: 76.17%\n",
      "[200] OOB Accuracy: 76.08%\n",
      "[210] OOB Accuracy: 76.05%\n",
      "[220] OOB Accuracy: 76.03%\n",
      "[230] OOB Accuracy: 76.36%\n",
      "[240] OOB Accuracy: 76.34%\n",
      "[250] OOB Accuracy: 76.08%\n",
      "[260] OOB Accuracy: 76.33%\n",
      "[270] OOB Accuracy: 76.22%\n",
      "[280] OOB Accuracy: 76.39%\n",
      "[290] OOB Accuracy: 76.36%\n",
      "[300] OOB Accuracy: 76.47%\n",
      "[310] OOB Accuracy: 76.19%\n",
      "[320] OOB Accuracy: 76.31%\n",
      "[330] OOB Accuracy: 76.25%\n",
      "[340] OOB Accuracy: 76.30%\n",
      "[350] OOB Accuracy: 76.42%\n",
      "[360] OOB Accuracy: 76.30%\n",
      "[370] OOB Accuracy: 76.36%\n",
      "[380] OOB Accuracy: 76.56%\n",
      "[390] OOB Accuracy: 76.52%\n",
      "[400] OOB Accuracy: 76.44%\n",
      "Model training completed.\n",
      "\n",
      "     Fold 1 Accuracy: 77.88%\n",
      "   Fold 2/5\n",
      "Training with OOB-based Early Stopping\n",
      "[10] OOB Accuracy: 68.08%\n",
      "[20] OOB Accuracy: 71.50%\n",
      "[30] OOB Accuracy: 73.00%\n",
      "[40] OOB Accuracy: 73.97%\n",
      "[50] OOB Accuracy: 74.69%\n",
      "[60] OOB Accuracy: 75.00%\n",
      "[70] OOB Accuracy: 75.44%\n",
      "[80] OOB Accuracy: 75.50%\n",
      "[90] OOB Accuracy: 75.69%\n",
      "[100] OOB Accuracy: 75.61%\n",
      "[110] OOB Accuracy: 75.78%\n",
      "[120] OOB Accuracy: 75.94%\n",
      "[130] OOB Accuracy: 76.00%\n",
      "[140] OOB Accuracy: 75.66%\n",
      "[150] OOB Accuracy: 75.59%\n",
      "[160] OOB Accuracy: 75.95%\n",
      "[170] OOB Accuracy: 76.05%\n",
      "[180] OOB Accuracy: 76.11%\n",
      "[190] OOB Accuracy: 75.94%\n",
      "[200] OOB Accuracy: 76.12%\n",
      "[210] OOB Accuracy: 76.23%\n",
      "[220] OOB Accuracy: 76.17%\n",
      "[230] OOB Accuracy: 76.19%\n",
      "[240] OOB Accuracy: 76.38%\n",
      "[250] OOB Accuracy: 76.09%\n",
      "[260] OOB Accuracy: 76.14%\n",
      "[270] OOB Accuracy: 76.12%\n",
      "[280] OOB Accuracy: 76.31%\n",
      "[290] OOB Accuracy: 76.53%\n",
      "[300] OOB Accuracy: 76.58%\n",
      "[310] OOB Accuracy: 76.47%\n",
      "[320] OOB Accuracy: 76.42%\n",
      "[330] OOB Accuracy: 76.41%\n",
      "[340] OOB Accuracy: 76.39%\n",
      "[350] OOB Accuracy: 76.62%\n",
      "[360] OOB Accuracy: 76.64%\n",
      "[370] OOB Accuracy: 76.73%\n",
      "[380] OOB Accuracy: 76.62%\n",
      "[390] OOB Accuracy: 76.61%\n",
      "[400] OOB Accuracy: 76.56%\n",
      "Model training completed.\n",
      "\n",
      "     Fold 2 Accuracy: 77.12%\n",
      "   Fold 3/5\n",
      "Training with OOB-based Early Stopping\n",
      "[10] OOB Accuracy: 67.45%\n",
      "[20] OOB Accuracy: 71.41%\n",
      "[30] OOB Accuracy: 73.58%\n",
      "[40] OOB Accuracy: 74.17%\n",
      "[50] OOB Accuracy: 75.12%\n",
      "[60] OOB Accuracy: 75.33%\n",
      "[70] OOB Accuracy: 75.66%\n",
      "[80] OOB Accuracy: 75.69%\n",
      "[90] OOB Accuracy: 75.72%\n",
      "[100] OOB Accuracy: 75.92%\n",
      "[110] OOB Accuracy: 75.88%\n",
      "[120] OOB Accuracy: 76.08%\n",
      "[130] OOB Accuracy: 76.03%\n",
      "[140] OOB Accuracy: 76.20%\n",
      "[150] OOB Accuracy: 76.20%\n",
      "[160] OOB Accuracy: 76.42%\n",
      "[170] OOB Accuracy: 76.64%\n",
      "[180] OOB Accuracy: 76.53%\n",
      "[190] OOB Accuracy: 76.52%\n",
      "[200] OOB Accuracy: 76.62%\n",
      "[210] OOB Accuracy: 76.56%\n",
      "[220] OOB Accuracy: 76.61%\n",
      "[230] OOB Accuracy: 76.55%\n",
      "[240] OOB Accuracy: 76.38%\n",
      "[250] OOB Accuracy: 76.36%\n",
      "[260] OOB Accuracy: 76.59%\n",
      "[270] OOB Accuracy: 76.69%\n",
      "[280] OOB Accuracy: 76.75%\n",
      "[290] OOB Accuracy: 76.75%\n",
      "[300] OOB Accuracy: 76.83%\n",
      "[310] OOB Accuracy: 76.80%\n",
      "[320] OOB Accuracy: 76.84%\n",
      "[330] OOB Accuracy: 76.64%\n",
      "[340] OOB Accuracy: 76.67%\n",
      "[350] OOB Accuracy: 76.73%\n",
      "[360] OOB Accuracy: 76.75%\n",
      "[370] OOB Accuracy: 76.77%\n",
      "[380] OOB Accuracy: 76.94%\n",
      "[390] OOB Accuracy: 76.80%\n",
      "[400] OOB Accuracy: 77.05%\n",
      "Model training completed.\n",
      "\n",
      "     Fold 3 Accuracy: 76.19%\n",
      "   Fold 4/5\n",
      "Training with OOB-based Early Stopping\n",
      "[10] OOB Accuracy: 68.11%\n",
      "[20] OOB Accuracy: 72.06%\n",
      "[30] OOB Accuracy: 72.94%\n",
      "[40] OOB Accuracy: 73.83%\n",
      "[50] OOB Accuracy: 73.98%\n",
      "[60] OOB Accuracy: 74.83%\n",
      "[70] OOB Accuracy: 74.77%\n",
      "[80] OOB Accuracy: 75.14%\n",
      "[90] OOB Accuracy: 75.64%\n",
      "[100] OOB Accuracy: 75.73%\n",
      "[110] OOB Accuracy: 75.86%\n",
      "[120] OOB Accuracy: 76.00%\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Hyperparameter Search\n",
    "param_grid = {\n",
    "    'max_depth': [20],\n",
    "    'min_samples_split': [2, 4],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "}\n",
    "k_list = [36, 38, 40, 42, 44]\n",
    "\n",
    "best_score = -1\n",
    "best_params = None\n",
    "best_k = None\n",
    "best_idx = None\n",
    "\n",
    "full_model = Model()\n",
    "full_model.fit(X_eng, y_full)\n",
    "final_importance = full_model.feature_importance_\n",
    "\n",
    "for k_feat in k_list:\n",
    "    sel_idx = np.argsort(final_importance)[::-1][:k_feat]\n",
    "    X_sel = X_eng[:, sel_idx]\n",
    "\n",
    "    for md in param_grid['max_depth']:\n",
    "        for mss in param_grid['min_samples_split']:\n",
    "            for msl in param_grid['min_samples_leaf']:\n",
    "                params = {'max_depth': md, 'min_samples_split': mss, 'min_samples_leaf': msl}\n",
    "                score = cross_val_score(X_sel, y_full, params, k=10)\n",
    "                print(f'k={k_feat} depth={md} split={mss} leaf={msl} -> CV Acc {score*100:.2f}%')\n",
    "\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = params\n",
    "                    best_k = k_feat\n",
    "                    best_idx = sel_idx\n",
    "\n",
    "final_model = Model()\n",
    "for k, v in best_params.items():\n",
    "    setattr(final_model, k, v)\n",
    "X_final = X_eng[:, best_idx]\n",
    "final_model.fit(X_final, y_full)\n",
    "\n",
    "print(f'\\nBest k = {best_k}')\n",
    "print(f'Best params = {best_params}')\n",
    "print(f'Best CV Accuracy = {best_score*100:.2f}%')\n",
    "\n",
    "feat_names = []\n",
    "\n",
    "for i in range(20):\n",
    "    feat_names.append(f'X{i}')\n",
    "\n",
    "for i in range(30):\n",
    "    feat_names.append(f'I{i}')\n",
    "\n",
    "for i in range(6):\n",
    "    feat_names.append(f'C{i}')\n",
    "\n",
    "print(f\"\\n Top {best_k} features selected by importance:\\n\")\n",
    "for rank, idx in enumerate(best_idx):\n",
    "    name = feat_names[idx] if idx < len(feat_names) else f'F{idx}'\n",
    "    score = final_importance[idx]\n",
    "    print(f\"{rank+1:2d}. {name:>4}  | importance = {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test Data\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_ids = test_df['ID'].values if 'ID' in test_df.columns else test_df.index\n",
    "X_test_raw = test_df.drop(columns=['ID'], errors='ignore')\n",
    "\n",
    "X_test_filled = X_test_raw.fillna(X_full.median(numeric_only=True))\n",
    "X_test_std = std(X_test_filled.values) \n",
    "\n",
    "X_test_eng = add_interactions_combined(X_test_std, top_feats, correlated_pairs)\n",
    "\n",
    "X_test_final = X_test_eng[:, best_idx]\n",
    "\n",
    "preds = final_model.predict(X_test_final)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'Potability': preds\n",
    "})\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Saved predictions to submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

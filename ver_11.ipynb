{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Data Loading ----\n",
    "train_df = pd.read_csv('train.csv')\n",
    "if 'ID' in train_df.columns:\n",
    "    train_df = train_df.drop(columns=['ID'])\n",
    "\n",
    "X_full = train_df.drop(columns=['Y'])\n",
    "y_full = train_df['Y'].values\n",
    "\n",
    "# Fill missing values and standardize\n",
    "X_filled = X_full.fillna(X_full.median(numeric_only=True))\n",
    "X_values = X_filled.values\n",
    "mu = X_values.mean(axis=0)\n",
    "sigma = X_values.std(axis=0) + 1e-8\n",
    "std = lambda a: (a - mu) / sigma\n",
    "X_values = std(X_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Feature Engineering ----\n",
    "def add_interactions(X):\n",
    "    X_new = X.copy()\n",
    "    interactions = [\n",
    "        (4, 9),\n",
    "        (3, 9),\n",
    "        (10, 16),\n",
    "        (11, 16),\n",
    "        (4, 10),\n",
    "        (6, 8),\n",
    "    ]\n",
    "    for i, j in interactions:\n",
    "        X_new = np.column_stack([X_new, X[:, i] * X[:, j]])\n",
    "    important_features = [11, 13, 15, 10, 6, 17]\n",
    "    for i in range(len(important_features) - 1):\n",
    "        for j in range(i + 1, len(important_features)):\n",
    "            fi, fj = important_features[i], important_features[j]\n",
    "            ratio = X[:, fi] / (X[:, fj] + 1e-8)\n",
    "            X_new = np.column_stack([X_new, ratio])\n",
    "    return X_new\n",
    "\n",
    "X_eng = add_interactions(X_values)\n",
    "\n",
    "# ---- Feature Selection ----\n",
    "def select_top_features(X, y, k=30):\n",
    "    corrs = []\n",
    "    for i in range(X.shape[1]):\n",
    "        c = np.corrcoef(X[:, i], y)[0, 1]\n",
    "        if np.isnan(c):\n",
    "            c = 0.0\n",
    "        corrs.append(abs(c))\n",
    "    indices = np.argsort(corrs)[::-1][:k]\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.n_estimators = 400\n",
    "        self.max_depth = 20\n",
    "        self.min_samples_split = 2\n",
    "        self.min_samples_leaf = 1\n",
    "        self.max_features = 'sqrt'\n",
    "        self.bootstrap = True\n",
    "        self.trees = []\n",
    "        self.feature_indices = []\n",
    "        self.oob_indices = []\n",
    "        self.patience = 10\n",
    "        self.best_oob = -1\n",
    "        self.no_improve = 0\n",
    "\n",
    "    def _gini_impurity(self, y):\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        p = np.sum(y == 1) / len(y)\n",
    "        return 2 * p * (1 - p)\n",
    "\n",
    "    def _information_gain(self, y, left_y, right_y):\n",
    "        n = len(y)\n",
    "        if n == 0:\n",
    "            return 0\n",
    "        n_left = len(left_y)\n",
    "        n_right = len(right_y)\n",
    "        parent_gini = self._gini_impurity(y)\n",
    "        left_gini = self._gini_impurity(left_y)\n",
    "        right_gini = self._gini_impurity(right_y)\n",
    "        weighted_gini = (n_left / n) * left_gini + (n_right / n) * right_gini\n",
    "        return parent_gini - weighted_gini\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        if (depth >= self.max_depth or\n",
    "            n_samples < self.min_samples_split or\n",
    "            len(np.unique(y)) == 1):\n",
    "            return {'leaf': True, 'prediction': np.round(np.mean(y))}\n",
    "        if self.max_features == 'sqrt':\n",
    "            max_features = int(np.sqrt(n_features))\n",
    "        else:\n",
    "            max_features = n_features\n",
    "        feature_indices = np.random.choice(n_features, max_features, replace=False)\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        for feature_idx in feature_indices:\n",
    "            thresholds = np.percentile(X[:, feature_idx], [10,25,50,75,90])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature_idx] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                if np.sum(left_mask) < self.min_samples_leaf or np.sum(right_mask) < self.min_samples_leaf:\n",
    "                    continue\n",
    "                gain = self._information_gain(y, y[left_mask], y[right_mask])\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = threshold\n",
    "        if best_feature is None:\n",
    "            return {'leaf': True, 'prediction': np.round(np.mean(y))}\n",
    "        left_mask = X[:, best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "        left_tree = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_tree = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "        return {\n",
    "            'leaf': False,\n",
    "            'feature': best_feature,\n",
    "            'threshold': best_threshold,\n",
    "            'left': left_tree,\n",
    "            'right': right_tree\n",
    "        }\n",
    "\n",
    "    def _predict_tree(self, tree, X):\n",
    "        if tree['leaf']:\n",
    "            return np.full(len(X), tree['prediction'])\n",
    "        predictions = np.zeros(len(X))\n",
    "        left_mask = X[:, tree['feature']] <= tree['threshold']\n",
    "        right_mask = ~left_mask\n",
    "        if np.sum(left_mask) > 0:\n",
    "            predictions[left_mask] = self._predict_tree(tree['left'], X[left_mask])\n",
    "        if np.sum(right_mask) > 0:\n",
    "            predictions[right_mask] = self._predict_tree(tree['right'], X[right_mask])\n",
    "        return predictions\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        for i in range(self.n_estimators):\n",
    "            if self.bootstrap:\n",
    "                indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "                X_bootstrap = X[indices]\n",
    "                y_bootstrap = y[indices]\n",
    "                oob_idx = np.setdiff1d(np.arange(n_samples), indices)\n",
    "                self.oob_indices.append(oob_idx)\n",
    "            else:\n",
    "                X_bootstrap = X\n",
    "                y_bootstrap = y\n",
    "                self.oob_indices.append(np.arange(n_samples))\n",
    "            tree = self._build_tree(X_bootstrap, y_bootstrap)\n",
    "            self.trees.append(tree)\n",
    "            if (i + 1) % 10 == 0:\n",
    "                oob_pred = self._get_oob_predictions(X, i + 1)\n",
    "                oob_acc = np.mean(oob_pred == y)\n",
    "                print(f'[{i + 1:3d}] OOB Accuracy = {oob_acc*100:.2f}%')\n",
    "                if oob_acc > self.best_oob + 1e-6:\n",
    "                    self.best_oob = oob_acc\n",
    "                    self.no_improve = 0\n",
    "                else:\n",
    "                    self.no_improve += 1\n",
    "                if self.no_improve >= self.patience:\n",
    "                    print('Early-stop triggered')\n",
    "                    break\n",
    "\n",
    "    def _get_oob_predictions(self, X, n_trees):\n",
    "        n_samples = X.shape[0]\n",
    "        oob_votes = np.zeros(n_samples)\n",
    "        oob_counts = np.zeros(n_samples)\n",
    "        for t in range(n_trees):\n",
    "            idx = self.oob_indices[t]\n",
    "            if idx.size == 0:\n",
    "                continue\n",
    "            preds = self._predict_tree(self.trees[t], X[idx])\n",
    "            oob_votes[idx] += preds\n",
    "            oob_counts[idx] += 1\n",
    "        mask = oob_counts > 0\n",
    "        oob_final = np.zeros(n_samples, dtype=int)\n",
    "        oob_final[mask] = (oob_votes[mask] / oob_counts[mask] > 0.5).astype(int)\n",
    "        return oob_final\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        predictions = np.zeros(n_samples)\n",
    "        for tree in self.trees:\n",
    "            predictions += self._predict_tree(tree, X)\n",
    "        return predictions / len(self.trees)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) > 0.5).astype(int)\n",
    "\n",
    "# ---- K-Fold Utilities ----\n",
    "def k_fold_indices(n_samples, k, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    indices = rng.permutation(n_samples)\n",
    "    fold_sizes = [n_samples // k] * k\n",
    "    for i in range(n_samples % k):\n",
    "        fold_sizes[i] += 1\n",
    "    folds = []\n",
    "    current = 0\n",
    "    for size in fold_sizes:\n",
    "        folds.append(indices[current: current + size])\n",
    "        current += size\n",
    "    return folds\n",
    "\n",
    "def cross_val_score(X, y, params, k=5):\n",
    "    folds = k_fold_indices(len(X), k)\n",
    "    scores = []\n",
    "    for i in range(k):\n",
    "        val_idx = folds[i]\n",
    "        train_idx = np.hstack([folds[j] for j in range(k) if j != i])\n",
    "        model = Model()\n",
    "        for key, value in params.items():\n",
    "            setattr(model, key, value)\n",
    "        model.fit(X[train_idx], y[train_idx])\n",
    "        preds = model.predict(X[val_idx])\n",
    "        score = np.mean(preds == y[val_idx])\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Hyperparameter Search ----\n",
    "param_grid = {\n",
    "    'max_depth': [15, 20],\n",
    "    'min_samples_split': [2, 4],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "}\n",
    "k_list = [20, 30, 40]\n",
    "\n",
    "best_score = -1\n",
    "best_params = None\n",
    "best_k = None\n",
    "\n",
    "for k_feat in k_list:\n",
    "    sel_idx = select_top_features(X_eng, y_full, k=k_feat)\n",
    "    X_sel = X_eng[:, sel_idx]\n",
    "    for md in param_grid['max_depth']:\n",
    "        for mss in param_grid['min_samples_split']:\n",
    "            for msl in param_grid['min_samples_leaf']:\n",
    "                params = {\n",
    "                    'max_depth': md,\n",
    "                    'min_samples_split': mss,\n",
    "                    'min_samples_leaf': msl,\n",
    "                }\n",
    "                score = cross_val_score(X_sel, y_full, params, k=5)\n",
    "                print(f'k={k_feat} depth={md} split={mss} leaf={msl} -> CV Acc {score*100:.2f}%')\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = params\n",
    "                    best_k = k_feat\n",
    "\n",
    "print('\nBest k:', best_k)\n",
    "print('Best params:', best_params)\n",
    "print('Best CV Accuracy: {:.2f}%'.format(best_score * 100))\n",
    "\n",
    "# ---- Train Final Model ----\n",
    "final_model = Model()\n",
    "for k, v in best_params.items():\n",
    "    setattr(final_model, k, v)\n",
    "best_idx = select_top_features(X_eng, y_full, k=best_k)\n",
    "X_best = X_eng[:, best_idx]\n",
    "final_model.fit(X_best, y_full)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

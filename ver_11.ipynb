{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Data Loading ----\n",
    "train_df = pd.read_csv('train.csv')\n",
    "if 'ID' in train_df.columns:\n",
    "    train_df = train_df.drop(columns=['ID'])\n",
    "\n",
    "X_full = train_df.drop(columns=['Y'])\n",
    "y_full = train_df['Y'].values\n",
    "\n",
    "# Fill missing values and standardize\n",
    "X_filled = X_full.fillna(X_full.median(numeric_only=True))\n",
    "X_values = X_filled.values\n",
    "mu = X_values.mean(axis=0)\n",
    "sigma = X_values.std(axis=0) + 1e-8\n",
    "std = lambda a: (a - mu) / sigma\n",
    "X_values = std(X_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# ---- Feature Engineering ----\n",
    "def add_interactions(X):\n",
    "    X_new = X.copy()\n",
    "    interactions = [\n",
    "        (4, 9),\n",
    "        (3, 9),\n",
    "        (10, 16),\n",
    "        (11, 16),\n",
    "        (4, 10),\n",
    "        (6, 8),\n",
    "    ]\n",
    "    for i, j in interactions:\n",
    "        X_new = np.column_stack([X_new, X[:, i] * X[:, j]])\n",
    "    \n",
    "    important_features = [17, 6, 16, 3, 11, 5]    \n",
    "    for i in range(len(important_features) - 1):\n",
    "        for j in range(i + 1, len(important_features)):\n",
    "            fi, fj = important_features[i], important_features[j]\n",
    "            ratio = X[:, fi] / (X[:, fj] + 1e-8)\n",
    "            X_new = np.column_stack([X_new, ratio])\n",
    "    \n",
    "    return X_new\n",
    "\n",
    "# ---- Feature Selection ----\n",
    "def select_top_features(X, y, k=30):\n",
    "    feature_names = []\n",
    "    corrs_pearson = []\n",
    "    corrs_spearman = []\n",
    "\n",
    "    for i in range(X.shape[1]):\n",
    "        pearson = np.corrcoef(X[:, i], y)[0, 1]\n",
    "        if np.isnan(pearson):\n",
    "            pearson = 0.0\n",
    "        spearman, _ = spearmanr(X[:, i], y)\n",
    "        if np.isnan(spearman):\n",
    "            spearman = 0.0\n",
    "\n",
    "        corrs_pearson.append(pearson)\n",
    "        corrs_spearman.append(spearman)\n",
    "        feature_names.append(f\"X{i}\" if i < X.shape[1] - 15 else f\"F{i}\")  # ê°„ì´ feature ì´ë¦„ ì„¤ì •\n",
    "\n",
    "    df_corr = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Pearson\": corrs_pearson,\n",
    "        \"Spearman\": corrs_spearman\n",
    "    })\n",
    "\n",
    "    print_sorted_correlations(df_corr)  # ì •ë ¬ ë° ì¶œë ¥\n",
    "    # ì ˆëŒ€ê°’ ê¸°ì¤€ìœ¼ë¡œ Pearson top-k ì¸ë±ìŠ¤ ì¶”ì¶œ\n",
    "    top_indices = np.argsort(np.abs(df_corr[\"Pearson\"].values))[::-1][:k]\n",
    "    return top_indices\n",
    "\n",
    "# ---- Correlation Printer ----\n",
    "def print_sorted_correlations(df):\n",
    "    print(\"ðŸ§ª Pearson ê¸°ì¤€ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬:\")\n",
    "    sorted_by_pearson = df.reindex(df['Pearson'].abs().sort_values().index).reset_index(drop=True)\n",
    "    print(sorted_by_pearson.to_string(index=False))\n",
    "\n",
    "    print(\"\\nðŸ§ª Spearman ê¸°ì¤€ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬:\")\n",
    "    sorted_by_spearman = df.reindex(df['Spearman'].abs().sort_values().index).reset_index(drop=True)\n",
    "    print(sorted_by_spearman.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.n_estimators = 400\n",
    "        self.max_depth = 20\n",
    "        self.min_samples_split = 2\n",
    "        self.min_samples_leaf = 1\n",
    "        self.max_features = 'sqrt'\n",
    "        self.bootstrap = True\n",
    "        self.trees = []\n",
    "        self.feature_indices = []\n",
    "        self.oob_indices = []\n",
    "        self.patience = 10\n",
    "        self.best_oob = -1\n",
    "        self.no_improve = 0\n",
    "\n",
    "    def _gini_impurity(self, y):\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        p = np.sum(y == 1) / len(y)\n",
    "        return 2 * p * (1 - p)\n",
    "\n",
    "    def _information_gain(self, y, left_y, right_y):\n",
    "        n = len(y)\n",
    "        if n == 0:\n",
    "            return 0\n",
    "        n_left = len(left_y)\n",
    "        n_right = len(right_y)\n",
    "        parent_gini = self._gini_impurity(y)\n",
    "        left_gini = self._gini_impurity(left_y)\n",
    "        right_gini = self._gini_impurity(right_y)\n",
    "        weighted_gini = (n_left / n) * left_gini + (n_right / n) * right_gini\n",
    "        return parent_gini - weighted_gini\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        if (depth >= self.max_depth or\n",
    "            n_samples < self.min_samples_split or\n",
    "            len(np.unique(y)) == 1):\n",
    "            return {'leaf': True, 'prediction': np.round(np.mean(y))}\n",
    "        if self.max_features == 'sqrt':\n",
    "            max_features = int(np.sqrt(n_features))\n",
    "        else:\n",
    "            max_features = n_features\n",
    "        feature_indices = np.random.choice(n_features, max_features, replace=False)\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        for feature_idx in feature_indices:\n",
    "            thresholds = np.percentile(X[:, feature_idx], [10,25,50,75,90])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature_idx] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                if np.sum(left_mask) < self.min_samples_leaf or np.sum(right_mask) < self.min_samples_leaf:\n",
    "                    continue\n",
    "                gain = self._information_gain(y, y[left_mask], y[right_mask])\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = threshold\n",
    "        if best_feature is None:\n",
    "            return {'leaf': True, 'prediction': np.round(np.mean(y))}\n",
    "        left_mask = X[:, best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "        left_tree = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_tree = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "        return {\n",
    "            'leaf': False,\n",
    "            'feature': best_feature,\n",
    "            'threshold': best_threshold,\n",
    "            'left': left_tree,\n",
    "            'right': right_tree\n",
    "        }\n",
    "\n",
    "    def _predict_tree(self, tree, X):\n",
    "        if tree['leaf']:\n",
    "            return np.full(len(X), tree['prediction'])\n",
    "        predictions = np.zeros(len(X))\n",
    "        left_mask = X[:, tree['feature']] <= tree['threshold']\n",
    "        right_mask = ~left_mask\n",
    "        if np.sum(left_mask) > 0:\n",
    "            predictions[left_mask] = self._predict_tree(tree['left'], X[left_mask])\n",
    "        if np.sum(right_mask) > 0:\n",
    "            predictions[right_mask] = self._predict_tree(tree['right'], X[right_mask])\n",
    "        return predictions\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        for i in range(self.n_estimators):\n",
    "            if self.bootstrap:\n",
    "                indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "                X_bootstrap = X[indices]\n",
    "                y_bootstrap = y[indices]\n",
    "                oob_idx = np.setdiff1d(np.arange(n_samples), indices)\n",
    "                self.oob_indices.append(oob_idx)\n",
    "            else:\n",
    "                X_bootstrap = X\n",
    "                y_bootstrap = y\n",
    "                self.oob_indices.append(np.arange(n_samples))\n",
    "            tree = self._build_tree(X_bootstrap, y_bootstrap)\n",
    "            self.trees.append(tree)\n",
    "            if (i + 1) % 10 == 0:\n",
    "                oob_pred = self._get_oob_predictions(X, i + 1)\n",
    "                oob_acc = np.mean(oob_pred == y)\n",
    "                print(f'[{i + 1:3d}] OOB Accuracy = {oob_acc*100:.2f}%')\n",
    "                if oob_acc > self.best_oob + 1e-6:\n",
    "                    self.best_oob = oob_acc\n",
    "                    self.no_improve = 0\n",
    "                else:\n",
    "                    self.no_improve += 1\n",
    "                if self.no_improve >= self.patience:\n",
    "                    print('Early-stop triggered')\n",
    "                    break\n",
    "\n",
    "    def _get_oob_predictions(self, X, n_trees):\n",
    "        n_samples = X.shape[0]\n",
    "        oob_votes = np.zeros(n_samples)\n",
    "        oob_counts = np.zeros(n_samples)\n",
    "        for t in range(n_trees):\n",
    "            idx = self.oob_indices[t]\n",
    "            if idx.size == 0:\n",
    "                continue\n",
    "            preds = self._predict_tree(self.trees[t], X[idx])\n",
    "            oob_votes[idx] += preds\n",
    "            oob_counts[idx] += 1\n",
    "        mask = oob_counts > 0\n",
    "        oob_final = np.zeros(n_samples, dtype=int)\n",
    "        oob_final[mask] = (oob_votes[mask] / oob_counts[mask] > 0.5).astype(int)\n",
    "        return oob_final\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        predictions = np.zeros(n_samples)\n",
    "        for tree in self.trees:\n",
    "            predictions += self._predict_tree(tree, X)\n",
    "        return predictions / len(self.trees)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) > 0.5).astype(int)\n",
    "\n",
    "# ---- K-Fold Utilities ----\n",
    "def k_fold_indices(n_samples, k, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    indices = rng.permutation(n_samples)\n",
    "    fold_sizes = [n_samples // k] * k\n",
    "    for i in range(n_samples % k):\n",
    "        fold_sizes[i] += 1\n",
    "    folds = []\n",
    "    current = 0\n",
    "    for size in fold_sizes:\n",
    "        folds.append(indices[current: current + size])\n",
    "        current += size\n",
    "    return folds\n",
    "\n",
    "def cross_val_score(X, y, params, k=5):\n",
    "    folds = k_fold_indices(len(X), k)\n",
    "    scores = []\n",
    "    for i in range(k):\n",
    "        val_idx = folds[i]\n",
    "        train_idx = np.hstack([folds[j] for j in range(k) if j != i])\n",
    "        model = Model()\n",
    "        for key, value in params.items():\n",
    "            setattr(model, key, value)\n",
    "        model.fit(X[train_idx], y[train_idx])\n",
    "        preds = model.predict(X[val_idx])\n",
    "        score = np.mean(preds == y[val_idx])\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10] OOB Accuracy = 65.22%\n",
      "[ 20] OOB Accuracy = 69.08%\n",
      "[ 30] OOB Accuracy = 70.37%\n",
      "[ 40] OOB Accuracy = 72.02%\n",
      "[ 50] OOB Accuracy = 72.55%\n",
      "[ 60] OOB Accuracy = 72.94%\n",
      "[ 70] OOB Accuracy = 73.65%\n",
      "[ 80] OOB Accuracy = 73.73%\n",
      "[ 90] OOB Accuracy = 74.35%\n",
      "[100] OOB Accuracy = 74.31%\n",
      "[110] OOB Accuracy = 74.69%\n",
      "[120] OOB Accuracy = 74.97%\n",
      "[130] OOB Accuracy = 74.78%\n",
      "[140] OOB Accuracy = 75.14%\n",
      "[150] OOB Accuracy = 75.08%\n",
      "[160] OOB Accuracy = 74.93%\n",
      "[170] OOB Accuracy = 75.17%\n",
      "[180] OOB Accuracy = 74.93%\n",
      "[190] OOB Accuracy = 75.02%\n",
      "[200] OOB Accuracy = 74.97%\n",
      "[210] OOB Accuracy = 74.97%\n",
      "[220] OOB Accuracy = 75.23%\n",
      "[230] OOB Accuracy = 75.14%\n",
      "[240] OOB Accuracy = 75.12%\n",
      "[250] OOB Accuracy = 75.10%\n",
      "[260] OOB Accuracy = 74.78%\n",
      "[270] OOB Accuracy = 75.00%\n",
      "[280] OOB Accuracy = 75.40%\n",
      "[290] OOB Accuracy = 75.36%\n",
      "[300] OOB Accuracy = 75.29%\n",
      "[310] OOB Accuracy = 75.30%\n",
      "[320] OOB Accuracy = 75.30%\n",
      "[330] OOB Accuracy = 75.55%\n",
      "[340] OOB Accuracy = 75.40%\n",
      "[350] OOB Accuracy = 75.27%\n",
      "[360] OOB Accuracy = 75.38%\n",
      "[370] OOB Accuracy = 75.57%\n",
      "[380] OOB Accuracy = 75.45%\n",
      "[390] OOB Accuracy = 75.40%\n",
      "[400] OOB Accuracy = 75.59%\n",
      "[ 10] OOB Accuracy = 65.65%\n",
      "[ 20] OOB Accuracy = 70.45%\n",
      "[ 30] OOB Accuracy = 72.23%\n",
      "[ 40] OOB Accuracy = 73.52%\n",
      "[ 50] OOB Accuracy = 74.29%\n",
      "[ 60] OOB Accuracy = 73.86%\n",
      "[ 70] OOB Accuracy = 74.37%\n",
      "[ 80] OOB Accuracy = 74.70%\n",
      "[ 90] OOB Accuracy = 74.67%\n",
      "[100] OOB Accuracy = 75.02%\n",
      "[110] OOB Accuracy = 74.95%\n",
      "[120] OOB Accuracy = 75.12%\n",
      "[130] OOB Accuracy = 75.40%\n",
      "[140] OOB Accuracy = 75.49%\n",
      "[150] OOB Accuracy = 75.57%\n",
      "[160] OOB Accuracy = 75.70%\n",
      "[170] OOB Accuracy = 75.53%\n",
      "[180] OOB Accuracy = 75.64%\n",
      "[190] OOB Accuracy = 75.77%\n",
      "[200] OOB Accuracy = 75.75%\n",
      "[210] OOB Accuracy = 75.72%\n",
      "[220] OOB Accuracy = 75.75%\n",
      "[230] OOB Accuracy = 76.04%\n",
      "[240] OOB Accuracy = 75.94%\n",
      "[250] OOB Accuracy = 75.94%\n",
      "[260] OOB Accuracy = 76.34%\n",
      "[270] OOB Accuracy = 76.37%\n",
      "[280] OOB Accuracy = 76.30%\n",
      "[290] OOB Accuracy = 76.32%\n",
      "[300] OOB Accuracy = 76.45%\n",
      "[310] OOB Accuracy = 76.60%\n",
      "[320] OOB Accuracy = 76.58%\n",
      "[330] OOB Accuracy = 76.60%\n",
      "[340] OOB Accuracy = 76.52%\n",
      "[350] OOB Accuracy = 76.43%\n",
      "[360] OOB Accuracy = 76.22%\n",
      "[370] OOB Accuracy = 76.30%\n",
      "[380] OOB Accuracy = 76.28%\n",
      "[390] OOB Accuracy = 76.41%\n",
      "[400] OOB Accuracy = 76.26%\n",
      "[ 10] OOB Accuracy = 66.50%\n",
      "[ 20] OOB Accuracy = 69.82%\n",
      "[ 30] OOB Accuracy = 71.63%\n",
      "[ 40] OOB Accuracy = 72.68%\n",
      "[ 50] OOB Accuracy = 73.32%\n",
      "[ 60] OOB Accuracy = 73.70%\n",
      "[ 70] OOB Accuracy = 74.20%\n",
      "[ 80] OOB Accuracy = 74.73%\n",
      "[ 90] OOB Accuracy = 74.93%\n",
      "[100] OOB Accuracy = 74.93%\n",
      "[110] OOB Accuracy = 75.14%\n",
      "[120] OOB Accuracy = 75.14%\n",
      "[130] OOB Accuracy = 75.23%\n",
      "[140] OOB Accuracy = 75.57%\n",
      "[150] OOB Accuracy = 75.44%\n",
      "[160] OOB Accuracy = 75.50%\n",
      "[170] OOB Accuracy = 75.50%\n",
      "[180] OOB Accuracy = 75.55%\n",
      "[190] OOB Accuracy = 75.78%\n",
      "[200] OOB Accuracy = 75.91%\n",
      "[210] OOB Accuracy = 75.89%\n",
      "[220] OOB Accuracy = 75.93%\n",
      "[230] OOB Accuracy = 75.97%\n",
      "[240] OOB Accuracy = 75.85%\n",
      "[250] OOB Accuracy = 75.93%\n",
      "[260] OOB Accuracy = 75.98%\n",
      "[270] OOB Accuracy = 76.19%\n",
      "[280] OOB Accuracy = 76.28%\n",
      "[290] OOB Accuracy = 76.32%\n",
      "[300] OOB Accuracy = 76.40%\n",
      "[310] OOB Accuracy = 76.30%\n",
      "[320] OOB Accuracy = 76.43%\n",
      "[330] OOB Accuracy = 76.34%\n",
      "[340] OOB Accuracy = 76.57%\n",
      "[350] OOB Accuracy = 76.49%\n",
      "[360] OOB Accuracy = 76.30%\n",
      "[370] OOB Accuracy = 76.19%\n",
      "[380] OOB Accuracy = 76.38%\n",
      "[390] OOB Accuracy = 76.40%\n",
      "[400] OOB Accuracy = 76.60%\n",
      "k=37 depth=20 split=2 leaf=1 -> CV Acc 76.25%\n",
      "[ 10] OOB Accuracy = 65.57%\n",
      "[ 20] OOB Accuracy = 68.59%\n",
      "[ 30] OOB Accuracy = 70.50%\n",
      "[ 40] OOB Accuracy = 71.95%\n",
      "[ 50] OOB Accuracy = 72.66%\n",
      "[ 60] OOB Accuracy = 73.60%\n",
      "[ 70] OOB Accuracy = 73.58%\n",
      "[ 80] OOB Accuracy = 74.10%\n",
      "[ 90] OOB Accuracy = 74.27%\n",
      "[100] OOB Accuracy = 74.57%\n",
      "[110] OOB Accuracy = 74.44%\n",
      "[120] OOB Accuracy = 74.25%\n",
      "[130] OOB Accuracy = 74.46%\n",
      "[140] OOB Accuracy = 74.59%\n",
      "[150] OOB Accuracy = 74.78%\n",
      "[160] OOB Accuracy = 74.87%\n",
      "[170] OOB Accuracy = 75.06%\n",
      "[180] OOB Accuracy = 75.27%\n",
      "[190] OOB Accuracy = 75.36%\n",
      "[200] OOB Accuracy = 75.29%\n",
      "[210] OOB Accuracy = 75.34%\n",
      "[220] OOB Accuracy = 75.14%\n",
      "[230] OOB Accuracy = 75.23%\n",
      "[240] OOB Accuracy = 75.42%\n",
      "[250] OOB Accuracy = 75.34%\n",
      "[260] OOB Accuracy = 75.23%\n",
      "[270] OOB Accuracy = 75.51%\n",
      "[280] OOB Accuracy = 75.44%\n",
      "[290] OOB Accuracy = 75.57%\n",
      "[300] OOB Accuracy = 75.59%\n",
      "[310] OOB Accuracy = 75.68%\n",
      "[320] OOB Accuracy = 75.55%\n",
      "[330] OOB Accuracy = 75.64%\n",
      "[340] OOB Accuracy = 75.77%\n",
      "[350] OOB Accuracy = 75.64%\n",
      "[360] OOB Accuracy = 75.90%\n",
      "[370] OOB Accuracy = 75.96%\n",
      "[380] OOB Accuracy = 76.07%\n",
      "[390] OOB Accuracy = 75.87%\n",
      "[400] OOB Accuracy = 75.96%\n",
      "[ 10] OOB Accuracy = 68.33%\n",
      "[ 20] OOB Accuracy = 71.24%\n",
      "[ 30] OOB Accuracy = 72.44%\n",
      "[ 40] OOB Accuracy = 73.58%\n",
      "[ 50] OOB Accuracy = 74.55%\n",
      "[ 60] OOB Accuracy = 74.46%\n",
      "[ 70] OOB Accuracy = 74.55%\n",
      "[ 80] OOB Accuracy = 74.78%\n",
      "[ 90] OOB Accuracy = 74.93%\n",
      "[100] OOB Accuracy = 75.23%\n",
      "[110] OOB Accuracy = 75.30%\n",
      "[120] OOB Accuracy = 75.45%\n",
      "[130] OOB Accuracy = 75.94%\n",
      "[140] OOB Accuracy = 76.09%\n",
      "[150] OOB Accuracy = 76.05%\n",
      "[160] OOB Accuracy = 76.15%\n",
      "[170] OOB Accuracy = 76.13%\n",
      "[180] OOB Accuracy = 76.09%\n",
      "[190] OOB Accuracy = 76.20%\n",
      "[200] OOB Accuracy = 76.04%\n",
      "[210] OOB Accuracy = 76.19%\n",
      "[220] OOB Accuracy = 76.24%\n",
      "[230] OOB Accuracy = 76.26%\n",
      "[240] OOB Accuracy = 76.19%\n",
      "[250] OOB Accuracy = 75.75%\n",
      "[260] OOB Accuracy = 75.96%\n",
      "[270] OOB Accuracy = 76.22%\n",
      "[280] OOB Accuracy = 76.15%\n",
      "[290] OOB Accuracy = 76.19%\n",
      "[300] OOB Accuracy = 76.49%\n",
      "[310] OOB Accuracy = 76.20%\n",
      "[320] OOB Accuracy = 76.15%\n",
      "[330] OOB Accuracy = 76.02%\n",
      "[340] OOB Accuracy = 76.11%\n",
      "[350] OOB Accuracy = 76.13%\n",
      "[360] OOB Accuracy = 76.22%\n",
      "[370] OOB Accuracy = 76.19%\n",
      "[380] OOB Accuracy = 76.22%\n",
      "[390] OOB Accuracy = 76.24%\n",
      "[400] OOB Accuracy = 76.39%\n",
      "Early-stop triggered\n",
      "[ 10] OOB Accuracy = 66.14%\n",
      "[ 20] OOB Accuracy = 69.40%\n",
      "[ 30] OOB Accuracy = 71.60%\n",
      "[ 40] OOB Accuracy = 72.52%\n",
      "[ 50] OOB Accuracy = 73.32%\n",
      "[ 60] OOB Accuracy = 73.43%\n",
      "[ 70] OOB Accuracy = 73.90%\n",
      "[ 80] OOB Accuracy = 74.17%\n",
      "[ 90] OOB Accuracy = 74.28%\n",
      "[100] OOB Accuracy = 74.54%\n",
      "[110] OOB Accuracy = 74.93%\n",
      "[120] OOB Accuracy = 74.77%\n",
      "[130] OOB Accuracy = 75.10%\n",
      "[140] OOB Accuracy = 74.99%\n",
      "[150] OOB Accuracy = 75.44%\n",
      "[160] OOB Accuracy = 75.20%\n",
      "[170] OOB Accuracy = 75.05%\n",
      "[180] OOB Accuracy = 75.18%\n",
      "[190] OOB Accuracy = 75.10%\n",
      "[200] OOB Accuracy = 75.31%\n",
      "[210] OOB Accuracy = 75.63%\n",
      "[220] OOB Accuracy = 75.68%\n",
      "[230] OOB Accuracy = 75.53%\n",
      "[240] OOB Accuracy = 75.53%\n",
      "[250] OOB Accuracy = 75.68%\n",
      "[260] OOB Accuracy = 75.44%\n",
      "[270] OOB Accuracy = 75.67%\n",
      "[280] OOB Accuracy = 75.63%\n",
      "[290] OOB Accuracy = 75.82%\n",
      "[300] OOB Accuracy = 75.80%\n",
      "[310] OOB Accuracy = 75.97%\n",
      "[320] OOB Accuracy = 76.08%\n",
      "[330] OOB Accuracy = 75.91%\n",
      "[340] OOB Accuracy = 76.04%\n",
      "[350] OOB Accuracy = 76.19%\n",
      "[360] OOB Accuracy = 76.02%\n",
      "[370] OOB Accuracy = 75.91%\n",
      "[380] OOB Accuracy = 75.93%\n",
      "[390] OOB Accuracy = 75.87%\n",
      "[400] OOB Accuracy = 75.95%\n",
      "k=38 depth=20 split=2 leaf=1 -> CV Acc 76.49%\n",
      "[ 10] OOB Accuracy = 65.54%\n",
      "[ 20] OOB Accuracy = 70.20%\n",
      "[ 30] OOB Accuracy = 71.69%\n",
      "[ 40] OOB Accuracy = 72.53%\n",
      "[ 50] OOB Accuracy = 73.24%\n",
      "[ 60] OOB Accuracy = 74.10%\n",
      "[ 70] OOB Accuracy = 74.20%\n",
      "[ 80] OOB Accuracy = 74.82%\n",
      "[ 90] OOB Accuracy = 74.82%\n",
      "[100] OOB Accuracy = 75.25%\n",
      "[110] OOB Accuracy = 74.95%\n",
      "[120] OOB Accuracy = 75.36%\n",
      "[130] OOB Accuracy = 75.34%\n",
      "[140] OOB Accuracy = 75.08%\n",
      "[150] OOB Accuracy = 75.14%\n",
      "[160] OOB Accuracy = 75.49%\n",
      "[170] OOB Accuracy = 75.21%\n",
      "[180] OOB Accuracy = 75.29%\n",
      "[190] OOB Accuracy = 75.45%\n",
      "[200] OOB Accuracy = 75.55%\n",
      "[210] OOB Accuracy = 75.64%\n",
      "[220] OOB Accuracy = 75.77%\n",
      "[230] OOB Accuracy = 75.72%\n",
      "[240] OOB Accuracy = 75.81%\n",
      "[250] OOB Accuracy = 75.75%\n",
      "[260] OOB Accuracy = 75.77%\n",
      "[270] OOB Accuracy = 76.05%\n",
      "[280] OOB Accuracy = 75.75%\n",
      "[290] OOB Accuracy = 75.75%\n",
      "[300] OOB Accuracy = 75.77%\n",
      "[310] OOB Accuracy = 75.89%\n",
      "[320] OOB Accuracy = 75.77%\n",
      "[330] OOB Accuracy = 75.66%\n",
      "[340] OOB Accuracy = 75.68%\n",
      "[350] OOB Accuracy = 75.92%\n",
      "[360] OOB Accuracy = 76.15%\n",
      "[370] OOB Accuracy = 75.89%\n",
      "[380] OOB Accuracy = 75.90%\n",
      "[390] OOB Accuracy = 75.92%\n",
      "[400] OOB Accuracy = 76.07%\n",
      "[ 10] OOB Accuracy = 67.41%\n",
      "[ 20] OOB Accuracy = 69.38%\n",
      "[ 30] OOB Accuracy = 70.90%\n",
      "[ 40] OOB Accuracy = 71.97%\n",
      "[ 50] OOB Accuracy = 73.20%\n",
      "[ 60] OOB Accuracy = 74.03%\n",
      "[ 70] OOB Accuracy = 74.52%\n",
      "[ 80] OOB Accuracy = 74.50%\n",
      "[ 90] OOB Accuracy = 74.59%\n",
      "[100] OOB Accuracy = 75.21%\n",
      "[110] OOB Accuracy = 75.19%\n",
      "[120] OOB Accuracy = 75.38%\n",
      "[130] OOB Accuracy = 75.49%\n",
      "[140] OOB Accuracy = 75.60%\n",
      "[150] OOB Accuracy = 75.77%\n",
      "[160] OOB Accuracy = 75.85%\n",
      "[170] OOB Accuracy = 75.96%\n",
      "[180] OOB Accuracy = 75.92%\n",
      "[190] OOB Accuracy = 76.09%\n",
      "[200] OOB Accuracy = 76.35%\n",
      "[210] OOB Accuracy = 76.20%\n",
      "[220] OOB Accuracy = 76.58%\n",
      "[230] OOB Accuracy = 76.73%\n",
      "[240] OOB Accuracy = 76.88%\n",
      "[250] OOB Accuracy = 76.73%\n",
      "[260] OOB Accuracy = 76.49%\n",
      "[270] OOB Accuracy = 76.52%\n",
      "[280] OOB Accuracy = 76.60%\n",
      "[290] OOB Accuracy = 76.62%\n",
      "[300] OOB Accuracy = 76.69%\n",
      "[310] OOB Accuracy = 76.80%\n",
      "[320] OOB Accuracy = 76.77%\n",
      "[330] OOB Accuracy = 76.67%\n",
      "[340] OOB Accuracy = 76.47%\n",
      "Early-stop triggered\n",
      "[ 10] OOB Accuracy = 66.55%\n",
      "[ 20] OOB Accuracy = 70.23%\n",
      "[ 30] OOB Accuracy = 72.22%\n",
      "[ 40] OOB Accuracy = 73.06%\n",
      "[ 50] OOB Accuracy = 73.90%\n",
      "[ 60] OOB Accuracy = 74.43%\n",
      "[ 70] OOB Accuracy = 74.33%\n",
      "[ 80] OOB Accuracy = 74.58%\n",
      "[ 90] OOB Accuracy = 75.03%\n",
      "[100] OOB Accuracy = 75.14%\n",
      "[110] OOB Accuracy = 74.99%\n",
      "[120] OOB Accuracy = 75.07%\n",
      "[130] OOB Accuracy = 75.78%\n",
      "[140] OOB Accuracy = 75.20%\n",
      "[150] OOB Accuracy = 75.31%\n",
      "[160] OOB Accuracy = 75.44%\n",
      "[170] OOB Accuracy = 75.48%\n",
      "[180] OOB Accuracy = 75.53%\n",
      "[190] OOB Accuracy = 75.55%\n",
      "[200] OOB Accuracy = 75.59%\n",
      "[210] OOB Accuracy = 75.85%\n",
      "[220] OOB Accuracy = 75.93%\n",
      "[230] OOB Accuracy = 75.97%\n",
      "[240] OOB Accuracy = 76.15%\n",
      "[250] OOB Accuracy = 76.36%\n",
      "[260] OOB Accuracy = 76.27%\n",
      "[270] OOB Accuracy = 76.40%\n",
      "[280] OOB Accuracy = 76.23%\n",
      "[290] OOB Accuracy = 76.30%\n",
      "[300] OOB Accuracy = 76.27%\n",
      "[310] OOB Accuracy = 76.17%\n",
      "[320] OOB Accuracy = 76.45%\n",
      "[330] OOB Accuracy = 76.40%\n",
      "[340] OOB Accuracy = 76.43%\n",
      "[350] OOB Accuracy = 76.55%\n",
      "[360] OOB Accuracy = 76.42%\n",
      "[370] OOB Accuracy = 76.57%\n",
      "[380] OOB Accuracy = 76.58%\n",
      "[390] OOB Accuracy = 76.58%\n",
      "[400] OOB Accuracy = 76.57%\n",
      "k=39 depth=20 split=2 leaf=1 -> CV Acc 76.31%\n",
      "\n",
      "Best k: 38\n",
      "Best params: {'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}\n",
      "Best CV Accuracy: 76.49%\n",
      "[ 10] OOB Accuracy = 67.34%\n",
      "[ 20] OOB Accuracy = 71.54%\n",
      "[ 30] OOB Accuracy = 72.67%\n",
      "[ 40] OOB Accuracy = 73.45%\n",
      "[ 50] OOB Accuracy = 74.00%\n",
      "[ 60] OOB Accuracy = 74.24%\n",
      "[ 70] OOB Accuracy = 74.85%\n",
      "[ 80] OOB Accuracy = 74.85%\n",
      "[ 90] OOB Accuracy = 75.46%\n",
      "[100] OOB Accuracy = 75.85%\n",
      "[110] OOB Accuracy = 75.96%\n",
      "[120] OOB Accuracy = 76.20%\n",
      "[130] OOB Accuracy = 76.28%\n",
      "[140] OOB Accuracy = 76.33%\n",
      "[150] OOB Accuracy = 76.56%\n",
      "[160] OOB Accuracy = 76.53%\n",
      "[170] OOB Accuracy = 76.51%\n",
      "[180] OOB Accuracy = 76.54%\n",
      "[190] OOB Accuracy = 76.81%\n",
      "[200] OOB Accuracy = 76.96%\n",
      "[210] OOB Accuracy = 76.79%\n",
      "[220] OOB Accuracy = 76.76%\n",
      "[230] OOB Accuracy = 77.11%\n",
      "[240] OOB Accuracy = 76.90%\n",
      "[250] OOB Accuracy = 76.85%\n",
      "[260] OOB Accuracy = 76.95%\n",
      "[270] OOB Accuracy = 76.96%\n",
      "[280] OOB Accuracy = 77.00%\n",
      "[290] OOB Accuracy = 76.96%\n",
      "[300] OOB Accuracy = 76.83%\n",
      "[310] OOB Accuracy = 76.89%\n",
      "[320] OOB Accuracy = 76.83%\n",
      "[330] OOB Accuracy = 76.95%\n",
      "Early-stop triggered\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ---- Hyperparameter Search ----\n",
    "param_grid = {\n",
    "    'max_depth': [20],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [1],\n",
    "}\n",
    "k_list = [37, 38, 39]\n",
    "\n",
    "best_score = -1\n",
    "best_params = None\n",
    "best_k = None\n",
    "\n",
    "for k_feat in k_list:\n",
    "    sel_idx = select_top_features(X_eng, y_full, k=k_feat)\n",
    "    X_sel = X_eng[:, sel_idx]\n",
    "    for md in param_grid['max_depth']:\n",
    "        for mss in param_grid['min_samples_split']:\n",
    "            for msl in param_grid['min_samples_leaf']:\n",
    "                params = {\n",
    "                    'max_depth': md,\n",
    "                    'min_samples_split': mss,\n",
    "                    'min_samples_leaf': msl,\n",
    "                }\n",
    "                score = cross_val_score(X_sel, y_full, params, k=3)\n",
    "                print(f'k={k_feat} depth={md} split={mss} leaf={msl} -> CV Acc {score*100:.2f}%')\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = params\n",
    "                    best_k = k_feat\n",
    "\n",
    "print('\\nBest k:', best_k)\n",
    "print('Best params:', best_params)\n",
    "print('Best CV Accuracy: {:.2f}%'.format(best_score * 100))\n",
    "\n",
    "# ---- Train Final Model ----\n",
    "final_model = Model()\n",
    "for k, v in best_params.items():\n",
    "    setattr(final_model, k, v)\n",
    "best_idx = select_top_features(X_eng, y_full, k=best_k)\n",
    "X_best = X_eng[:, best_idx]\n",
    "final_model.fit(X_best, y_full)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

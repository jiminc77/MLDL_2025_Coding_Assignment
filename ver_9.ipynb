{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":103219,"databundleVersionId":12462890,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv('/kaggle/input/mldl-2025/train.csv')\n\nif 'ID' in df.columns:\n    df = df.drop(columns=['ID'])\n\nfeatures = df.drop(columns=['Y'])\nX = features.fillna(features.median(numeric_only=True)).values\ny = df['Y'].values\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=32, stratify=y)\n\nmu, sigma = X_train.mean(axis=0), X_train.std(axis=0) + 1e-8\nstd = lambda a: (a - mu) / sigma\nX_train_std, X_test_std = std(X_train), std(X_test)\n\ndef add_interactions(X):\n    \"\"\"Add interaction features based on correlation analysis\"\"\"\n    X_new = X.copy()\n    \n    # Top correlated pairs from the analysis\n    interactions = [\n        (4, 9),   # 0.787 correlation\n        (3, 9),   # 0.755 correlation\n        (10, 16), # 0.695 correlation\n        (11, 16), # 0.677 correlation\n        (4, 10),  # 0.667 correlation\n        (6, 8),   # 0.588 correlation\n    ]\n    \n    # Add multiplication interactions\n    for i, j in interactions:\n        X_new = np.column_stack([X_new, X[:, i] * X[:, j]])\n    \n    # Add ratio features for features with high individual importance\n    important_features = [11, 13, 15, 10, 6, 17]\n    for i in range(len(important_features)-1):\n        for j in range(i+1, len(important_features)):\n            fi, fj = important_features[i], important_features[j]\n            # Avoid division by zero\n            ratio = X[:, fi] / (X[:, fj] + 1e-8)\n            X_new = np.column_stack([X_new, ratio])\n    \n    return X_new\n\n# Apply feature engineering\nX_train_eng = add_interactions(X_train_std)\nX_test_eng = add_interactions(X_test_std)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-04T10:41:12.121028Z","iopub.execute_input":"2025-06-04T10:41:12.121344Z","iopub.status.idle":"2025-06-04T10:41:12.235723Z","shell.execute_reply.started":"2025-06-04T10:41:12.121319Z","shell.execute_reply":"2025-06-04T10:41:12.234517Z"}},"outputs":[],"execution_count":129},{"cell_type":"code","source":"class Model:\n    def __init__(self):\n        # Random Forest parameters\n        self.n_estimators = 400\n        self.max_depth = 20\n        self.min_samples_split = 2\n        self.min_samples_leaf = 1\n        self.max_features = 'sqrt'  # sqrt of total features\n        self.bootstrap = True\n        \n        self.trees = []\n        self.feature_indices = []\n        \n        self.oob_indices = []\n        self.patience = 10\n        self.best_oob = -1\n        self.no_improve = 0\n        \n    def _gini_impurity(self, y):\n        \"\"\"Calculate Gini impurity\"\"\"\n        if len(y) == 0:\n            return 0\n        p = np.sum(y == 1) / len(y)\n        return 2 * p * (1 - p)\n    \n    def _information_gain(self, y, left_y, right_y):\n        \"\"\"Calculate information gain\"\"\"\n        n = len(y)\n        if n == 0:\n            return 0\n        \n        n_left = len(left_y)\n        n_right = len(right_y)\n        \n        parent_gini = self._gini_impurity(y)\n        left_gini = self._gini_impurity(left_y)\n        right_gini = self._gini_impurity(right_y)\n        \n        weighted_gini = (n_left / n) * left_gini + (n_right / n) * right_gini\n        return parent_gini - weighted_gini\n    \n    def _build_tree(self, X, y, depth=0):\n        \"\"\"Build a decision tree recursively\"\"\"\n        n_samples, n_features = X.shape\n        \n        # Stopping criteria\n        if (depth >= self.max_depth or \n            n_samples < self.min_samples_split or\n            len(np.unique(y)) == 1):\n            # Return leaf node with majority class\n            return {'leaf': True, 'prediction': np.round(np.mean(y))}\n        \n        # Feature subsampling\n        if self.max_features == 'sqrt':\n            max_features = int(np.sqrt(n_features))\n        else:\n            max_features = n_features\n        \n        feature_indices = np.random.choice(n_features, max_features, replace=False)\n        \n        # Find best split\n        best_gain = -1\n        best_feature = None\n        best_threshold = None\n        \n        for feature_idx in feature_indices:\n            # Try multiple threshold candidates\n            thresholds = np.percentile(X[:, feature_idx], [10, 25, 50, 75, 90])\n            \n            for threshold in thresholds:\n                left_mask = X[:, feature_idx] <= threshold\n                right_mask = ~left_mask\n                \n                if np.sum(left_mask) < self.min_samples_leaf or np.sum(right_mask) < self.min_samples_leaf:\n                    continue\n                \n                gain = self._information_gain(y, y[left_mask], y[right_mask])\n                \n                if gain > best_gain:\n                    best_gain = gain\n                    best_feature = feature_idx\n                    best_threshold = threshold\n        \n        # If no good split found\n        if best_feature is None:\n            return {'leaf': True, 'prediction': np.round(np.mean(y))}\n        \n        # Split data\n        left_mask = X[:, best_feature] <= best_threshold\n        right_mask = ~left_mask\n        \n        # Build subtrees\n        left_tree = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n        right_tree = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n        \n        return {\n            'leaf': False,\n            'feature': best_feature,\n            'threshold': best_threshold,\n            'left': left_tree,\n            'right': right_tree\n        }\n    \n    def _predict_tree(self, tree, X):\n        \"\"\"Make predictions with a single tree\"\"\"\n        if tree['leaf']:\n            return np.full(len(X), tree['prediction'])\n        \n        predictions = np.zeros(len(X))\n        left_mask = X[:, tree['feature']] <= tree['threshold']\n        right_mask = ~left_mask\n        \n        if np.sum(left_mask) > 0:\n            predictions[left_mask] = self._predict_tree(tree['left'], X[left_mask])\n        if np.sum(right_mask) > 0:\n            predictions[right_mask] = self._predict_tree(tree['right'], X[right_mask])\n        \n        return predictions\n    \n    def fit(self, X, y):\n        \"\"\"Fit Random Forest model\"\"\"\n        n_samples = X.shape[0]\n        \n        for i in range(self.n_estimators):\n            # Bootstrap sampling\n            if self.bootstrap:\n                indices = np.random.choice(n_samples, n_samples, replace=True)\n                X_bootstrap = X[indices]\n                y_bootstrap = y[indices]\n                oob_idx = np.setdiff1d(np.arange(n_samples), indices)\n                self.oob_indices.append(oob_idx)\n            else:\n                X_bootstrap = X\n                y_bootstrap = y\n                self.oob_indices.append(np.arange(n_samples))\n            \n            # Build tree\n            tree = self._build_tree(X_bootstrap, y_bootstrap)\n            self.trees.append(tree)\n            \n            # Progress update\n            if (i + 1) % 10 == 0:                     \n                oob_pred = self._get_oob_predictions(X, i + 1)\n                oob_acc = np.mean(oob_pred == y)\n                print(f\"[{i + 1:3d}] OOB Accuracy = {oob_acc*100:.2f}%\")\n\n                ### MOD 1-4: 조기 중단\n                if oob_acc > self.best_oob + 1e-6:\n                    self.best_oob = oob_acc\n                    self.no_improve = 0\n                else:\n                    self.no_improve += 1\n                if self.no_improve >= self.patience:\n                    print(\"Early-stop triggered\")\n                    break\n    \n    def _get_oob_predictions(self, X, n_trees):\n        n_samples = X.shape[0]\n        oob_votes = np.zeros(n_samples)\n        oob_counts = np.zeros(n_samples)\n\n        for t in range(n_trees):\n            idx = self.oob_indices[t]                # ### MOD 1-5\n            if idx.size == 0:\n                continue\n            preds = self._predict_tree(self.trees[t], X[idx])\n            oob_votes[idx] += preds\n            oob_counts[idx] += 1\n\n        mask = oob_counts > 0\n        oob_final = np.zeros(n_samples, dtype=int)\n        oob_final[mask] = (oob_votes[mask] / oob_counts[mask] > 0.5).astype(int)\n        return oob_final\n    \n    def predict_proba(self, X):\n        \"\"\"Get probability predictions\"\"\"\n        n_samples = X.shape[0]\n        predictions = np.zeros(n_samples)\n        \n        for tree in self.trees:\n            predictions += self._predict_tree(tree, X)\n        \n        return predictions / len(self.trees)\n    \n    def predict(self, X):\n        \"\"\"Make final predictions\"\"\"\n        probas = self.predict_proba(X)\n        return (probas > 0.5).astype(int)\n        \n\"\"\"\n\n# ================================\n# Parameter Grid Search 자동화\n# ================================\n\nmax_depth_list = [5, 10, 15, 20]\nmin_samples_split_list = [2, 5, 10]\nmin_samples_leaf_list = [1, 2, 4, 8]\n\n\nfor md in max_depth_list:\n    for mss in min_samples_split_list:\n        for msl in min_samples_leaf_list:\n            model = Model()\n            model.n_estimators = 300\n            model.max_depth = md\n            model.min_samples_split = mss\n            model.min_samples_leaf = msl\n            model.max_features = 'sqrt'\n            model.bootstrap = True\n            \n            print(f\"\\n--- Parameters: max_depth={md}, min_samples_split={mss}, min_samples_leaf={msl} ---\")\n            \n            model.fit(X_train, y_train)\n\n            val_preds = model.predict(X_test)\n            val_accuracy = np.mean(val_preds == y_test)\n            print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T10:26:44.148446Z","iopub.execute_input":"2025-06-04T10:26:44.149292Z","iopub.status.idle":"2025-06-04T10:26:44.177199Z","shell.execute_reply.started":"2025-06-04T10:26:44.149265Z","shell.execute_reply":"2025-06-04T10:26:44.176131Z"}},"outputs":[{"execution_count":127,"output_type":"execute_result","data":{"text/plain":"'\\n\\n# ================================\\n# Parameter Grid Search 자동화\\n# ================================\\n\\nmax_depth_list = [5, 10, 15, 20]\\nmin_samples_split_list = [2, 5, 10]\\nmin_samples_leaf_list = [1, 2, 4, 8]\\n\\n\\nfor md in max_depth_list:\\n    for mss in min_samples_split_list:\\n        for msl in min_samples_leaf_list:\\n            model = Model()\\n            model.n_estimators = 300\\n            model.max_depth = md\\n            model.min_samples_split = mss\\n            model.min_samples_leaf = msl\\n            model.max_features = \\'sqrt\\'\\n            model.bootstrap = True\\n            \\n            print(f\"\\n--- Parameters: max_depth={md}, min_samples_split={mss}, min_samples_leaf={msl} ---\")\\n            \\n            model.fit(X_train, y_train)\\n\\n            val_preds = model.predict(X_test)\\n            val_accuracy = np.mean(val_preds == y_test)\\n            print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\\n'"},"metadata":{}}],"execution_count":127},{"cell_type":"code","source":"# Instantiate and train\nm = Model()\nm.fit(X_train_eng, y_train)\n\n# Predict and evaluate\npredictions = m.predict(X_test_eng)\n\n# Accuracy\naccuracy = np.mean(predictions == y_test)\nprint(f\"Validation Accuracy: {accuracy * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T10:26:50.036835Z","iopub.execute_input":"2025-06-04T10:26:50.037205Z","iopub.status.idle":"2025-06-04T10:41:12.119472Z","shell.execute_reply.started":"2025-06-04T10:26:50.037178Z","shell.execute_reply":"2025-06-04T10:41:12.117944Z"}},"outputs":[{"name":"stdout","text":"[ 10] OOB Accuracy = 67.12%\n[ 20] OOB Accuracy = 69.50%\n[ 30] OOB Accuracy = 71.11%\n[ 40] OOB Accuracy = 71.84%\n[ 50] OOB Accuracy = 72.52%\n[ 60] OOB Accuracy = 72.89%\n[ 70] OOB Accuracy = 73.56%\n[ 80] OOB Accuracy = 73.89%\n[ 90] OOB Accuracy = 74.12%\n[100] OOB Accuracy = 74.42%\n[110] OOB Accuracy = 74.39%\n[120] OOB Accuracy = 74.30%\n[130] OOB Accuracy = 74.22%\n[140] OOB Accuracy = 74.41%\n[150] OOB Accuracy = 74.62%\n[160] OOB Accuracy = 74.73%\n[170] OOB Accuracy = 74.88%\n[180] OOB Accuracy = 74.88%\n[190] OOB Accuracy = 74.97%\n[200] OOB Accuracy = 74.91%\n[210] OOB Accuracy = 74.88%\n[220] OOB Accuracy = 74.92%\n[230] OOB Accuracy = 75.14%\n[240] OOB Accuracy = 75.08%\n[250] OOB Accuracy = 75.11%\n[260] OOB Accuracy = 75.34%\n[270] OOB Accuracy = 75.38%\n[280] OOB Accuracy = 75.25%\n[290] OOB Accuracy = 75.39%\n[300] OOB Accuracy = 75.44%\n[310] OOB Accuracy = 75.38%\n[320] OOB Accuracy = 75.44%\n[330] OOB Accuracy = 75.31%\n[340] OOB Accuracy = 75.53%\n[350] OOB Accuracy = 75.53%\n[360] OOB Accuracy = 75.38%\n[370] OOB Accuracy = 75.47%\n[380] OOB Accuracy = 75.52%\n[390] OOB Accuracy = 75.61%\n[400] OOB Accuracy = 75.45%\nValidation Accuracy: 75.88%\n","output_type":"stream"}],"execution_count":128},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/mldl-2025/test.csv')\ntest_ids = test_df.index  # use row index as ID\n\nX_test = test_df.values\n\n# Predict\npreds = m.predict(X_test)\n# preds = svm.predict(test_df.values)\n\n# ===== Create and Save Submission =====\nsubmission_df = pd.DataFrame({\n    'ID': test_ids,\n    'Potability': preds\n})\n\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"Saved predictions to submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T09:20:21.057333Z","iopub.status.idle":"2025-06-04T09:20:21.057714Z","shell.execute_reply.started":"2025-06-04T09:20:21.057538Z","shell.execute_reply":"2025-06-04T09:20:21.057551Z"}},"outputs":[],"execution_count":null}]}